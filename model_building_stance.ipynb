{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "294CcummpWl2",
        "outputId": "b6146a3d-18b7-4510-ab60-2bb5d8e82f2b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32me:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\model_building.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/model_building.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/model_building.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR8uhavw73yb",
        "outputId": "d483f37b-64a5-4dda-8d4a-5e50dd066b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nlp_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/nlp_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U139J06L2VSB",
        "outputId": "f678c9c5-9f66-471f-aca6-3abc474a0923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic-stopwords in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from arabic-stopwords) (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic>=0.6.2->arabic-stopwords) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: qalsadi in /usr/local/lib/python3.8/dist-packages (0.4.5)\n",
            "Requirement already satisfied: naftawayh>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: Arabic-Stopwords>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: alyahmor>=0.1 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.1.5)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.16.0)\n",
            "Requirement already satisfied: pickledb>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.9.2)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.6.15)\n",
            "Requirement already satisfied: arramooz-pysqlite>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: tashaphyne>=0.3.4.1 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.15.0)\n",
            "Requirement already satisfied: libqutrub>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install arabic-stopwords\n",
        "!pip install qalsadi\n",
        "!pip install pyarabic\n",
        "!pip install gensim\n",
        "!pip install top2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giRMTMSppgrI",
        "outputId": "1e5469a5-198b-49d3-ee15-ba7a3d0fde3e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string\n",
        "import qalsadi.lemmatizer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from pyarabic.araby import tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "from gru_model import ArabicDataset, Classifier, evaluate, train\n",
        "from pre_processing_post import processPost\n",
        "from feature_extraction import get_ngram_features, get_word_embedding_features, avg_word_vector, get_word_count_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8V_Ralq6pWl4"
      },
      "outputs": [],
      "source": [
        "# needed functions\n",
        "def print_report(pipe, x_test, y_test):\n",
        "    y_pred = pipe.predict(x_test)\n",
        "    report = metrics.classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EsnK9zupWl5"
      },
      "source": [
        "# Read train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "dlD_8DNapWl6",
        "outputId": "5c801c19-c001-4554-88ba-8c560c4914ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('./DataSet/train.csv',sep=',',header=0)\n",
        "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "a-m343EOpWl8",
        "outputId": "a8d192b6-009a-499f-8545-bc1903d0ca9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...         1       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...         2       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...         2       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...         1       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...         4       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...         2       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...         2       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...         4       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...         9       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...         2       1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#remove first row that has the header\n",
        "train_data['category'] = train_data['category'].astype('category').cat.codes\n",
        "train_data.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Po_uSrO-T6sS"
      },
      "source": [
        "## Over Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jf6OGm-JNNAm"
      },
      "outputs": [],
      "source": [
        "# !pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AMwOtKGOpWl8"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "# train_data=train_data.drop('category',axis=1)\n",
        "# y=train_data['stance']\n",
        "# print(Counter(train_data['stance']))\n",
        "# train_data=train_data.drop('stance',axis=1)\n",
        "# # define oversampling strategy\n",
        "# oversample = RandomOverSampler(random_state=3)\n",
        "# # fit and apply the transform\n",
        "# train_data[\"text\"], train_data['stance'] = oversample.fit_resample(train_data, y)\n",
        "# print(Counter(train_data['stance']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3SV1t53pWl9"
      },
      "source": [
        "# Pre-Processing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWbuBoDgpWl9",
        "outputId": "dfe81e23-b7d5-4c71-a7a7-6202ef2aa394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية لقاح لعد ما اتابع الاخبار هم بكل مجالاتهم متفوقين وراح يطلع اللقاح قريباً؟<LF>#دعبول_دومه_مسحول\n",
            "دعبول حضر من نت طلب قائد دول إسلام قاح عد تابع اخبار مجال متفوق طلع قاح قريبا دعبول دوم مسحول\n"
          ]
        }
      ],
      "source": [
        "print(train_data[\"text\"][9])\n",
        "train_data[\"text\"] = train_data['text'].apply(lambda x: processPost(x))\n",
        "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))\n",
        "print(train_data[\"text\"][9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f26urbLgkiKi"
      },
      "source": [
        "## Ara2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EfJCUEzEkpA-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'ar_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# load AraVec Spacy model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PqSckdSVkvrh"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing Class\n",
        "class Preprocessor:\n",
        "    def __init__(self, tokenizer, **cfg):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, text):\n",
        "        return self.tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cUqO4-OFk3Ch"
      },
      "outputs": [],
      "source": [
        "# Apply the `Preprocessor` Class\n",
        "nlp.tokenizer = Preprocessor(nlp.tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c7WB_vA_sIO"
      },
      "source": [
        "## create vocablary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RcoWDCqQ_w5n"
      },
      "outputs": [],
      "source": [
        "train_data_tokenized = train_data['text'].apply(tokenize)\n",
        "test_data_tokenized = test_data['text'].apply(tokenize)\n",
        "#merge all the sentences in one list\n",
        "vocab = [item for sublist in train_data_tokenized for item in sublist]\n",
        "vocab = list(set(vocab))\n",
        "vocab.append('<فراغ>') \n",
        "vocab.insert(0, '<مجهول>') \n",
        "word2index = {word: i for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the word2index dictionary\n",
        "with open('./vocab/stance/word2index.pickle', 'wb') as handle:\n",
        "    pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku3lg9H8pWl-"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfl03iEpWl-"
      },
      "source": [
        "## 1. TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "p1iEoxZepWl_",
        "outputId": "890cefd1-4d2f-4349-f1f9-fa27aafea987"
      },
      "outputs": [],
      "source": [
        "ngramdata_features, word_vectorizer = get_ngram_features(train_data)\n",
        "ngramdata_features.head()\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/TFIDFVectorizer.sav'\n",
        "pickle.dump(word_vectorizer, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rJZAOHGpWl_"
      },
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUdWIXKjpWl_",
        "outputId": "ffb4a071-f537-4938-b39a-35fbfddac3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.23      0.59      0.33        70\n",
            "           0       0.29      0.58      0.38       126\n",
            "           1       0.96      0.67      0.79       804\n",
            "\n",
            "    accuracy                           0.66      1000\n",
            "   macro avg       0.49      0.61      0.50      1000\n",
            "weighted avg       0.82      0.66      0.71      1000\n",
            "\n",
            "accuracy: 0.656\n"
          ]
        }
      ],
      "source": [
        "clf = SVC(kernel='linear', class_weight=\"balanced\")\n",
        "pipe_tfidf = make_pipeline(word_vectorizer, clf)\n",
        "pipe_tfidf.fit(train_data['text'], train_data['stance'])\n",
        "print_report(pipe_tfidf, test_data['text'], test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/LinearSVC_tfidf.sav'\n",
        "pickle.dump(pipe_tfidf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wMiuOYKLpWmA"
      },
      "source": [
        "RandomForest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxs8cWCrpWmA",
        "outputId": "ed301a76-ef87-4ebf-e862-f80e1fbeadba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.33      0.16      0.21        70\n",
            "           0       0.45      0.28      0.34       126\n",
            "           1       0.85      0.94      0.89       804\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.54      0.46      0.48      1000\n",
            "weighted avg       0.76      0.80      0.78      1000\n",
            "\n",
            "accuracy: 0.801\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_tfidf = rf.fit(X_train_tfidf, train_data['stance'])\n",
        "y_pred = rf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print_report(rf_tfidf, X_test_tfidf, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/RandomForest_tfidf.sav'\n",
        "pickle.dump(rf_tfidf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.30      0.44      0.36        70\n",
            "           0       0.30      0.63      0.41       126\n",
            "           1       0.94      0.74      0.83       804\n",
            "\n",
            "    accuracy                           0.71      1000\n",
            "   macro avg       0.52      0.61      0.53      1000\n",
            "weighted avg       0.82      0.71      0.74      1000\n",
            "\n",
            "accuracy: 0.707\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "CNB_clr = ComplementNB()\n",
        "CNB_clr_tfidf = CNB_clr.fit(X_train_tfidf, train_data['stance'])\n",
        "print_report(CNB_clr_tfidf, X_test_tfidf, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/NaiveBayes_tfidf.sav'\n",
        "pickle.dump(CNB_clr, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_word_features, test_word_features, word_counter_vectorizer = get_word_count_features(train_data, test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LinearSVC Classifire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.21      0.49      0.30        70\n",
            "           0       0.29      0.54      0.38       126\n",
            "           1       0.94      0.71      0.81       804\n",
            "\n",
            "    accuracy                           0.68      1000\n",
            "   macro avg       0.48      0.58      0.50      1000\n",
            "weighted avg       0.81      0.68      0.72      1000\n",
            "\n",
            "accuracy: 0.675\n"
          ]
        }
      ],
      "source": [
        "clf = SVC(kernel=\"linear\",class_weight=\"balanced\", probability=True) #{-1:0.66, 0:0.29, 1:0.05}\n",
        "clf.fit(train_word_features, train_data['stance'])\n",
        "print_report(clf, test_word_features, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/LinearSVC_BoW.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomForest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.31      0.16      0.21        70\n",
            "           0       0.44      0.32      0.37       126\n",
            "           1       0.86      0.93      0.89       804\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.54      0.47      0.49      1000\n",
            "weighted avg       0.77      0.80      0.78      1000\n",
            "\n",
            "accuracy: 0.801\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_Bow = rf.fit(train_word_features, train_data['stance'])\n",
        "\n",
        "print_report(rf_Bow, test_word_features, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/RandomForest_Bow.sav'\n",
        "pickle.dump(rf_Bow, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.26      0.40      0.32        70\n",
            "           0       0.30      0.67      0.42       126\n",
            "           1       0.94      0.72      0.82       804\n",
            "\n",
            "    accuracy                           0.69      1000\n",
            "   macro avg       0.50      0.60      0.52      1000\n",
            "weighted avg       0.82      0.69      0.73      1000\n",
            "\n",
            "accuracy: 0.693\n"
          ]
        }
      ],
      "source": [
        "CNB_clr = ComplementNB()\n",
        "CNB_clr_BoW = CNB_clr.fit(train_word_features, train_data['stance'])\n",
        "print_report(CNB_clr_BoW,test_word_features, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/NaiveBayes_BoW.sav'\n",
        "pickle.dump(CNB_clr, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeKP3mYvpWmA"
      },
      "source": [
        "## 3.CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_vect_avg, X_test_vect_avg = get_word_embedding_features(train_data, test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00        70\n",
            "           0       0.00      0.00      0.00       126\n",
            "           1       0.80      1.00      0.89       804\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.27      0.33      0.30      1000\n",
            "weighted avg       0.65      0.80      0.72      1000\n",
            "\n",
            "accuracy: 0.804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC(class_weight=\"balanced\") #{-1:0.66, 0:0.29, 1:0.05}\n",
        "clf.fit(X_train_vect_avg, train_data['stance'])\n",
        "print_report(clf, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/LinearSVC_CBOW.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.36      0.06      0.10        70\n",
            "           0       0.41      0.09      0.14       126\n",
            "           1       0.82      0.98      0.89       804\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.53      0.37      0.38      1000\n",
            "weighted avg       0.74      0.80      0.74      1000\n",
            "\n",
            "accuracy: 0.803\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
        "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/RandomForest_CBOW.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.Ara2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQacWVpg01Q-",
        "outputId": "9b4558b1-951e-4de2-9fb6-ed28a2479908"
      },
      "outputs": [],
      "source": [
        "train_data_embeddings = np.array([np.array([nlp(i).vector for i in ls if i in vocab]) for ls in train_data[\"text\"]], dtype=object)\n",
        "test_data_embeddings = np.array([np.array([nlp(i).vector for i in ls if i in vocab]) for ls in test_data[\"text\"]], dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vLxZAs4QpWmA"
      },
      "outputs": [],
      "source": [
        "X_train_vect_avg, X_test_vect_avg = avg_word_vector(train_data_embeddings, test_data_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN3Nc-kKpWmB"
      },
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQzpnk2SpWmB",
        "outputId": "6ee5af9d-bf13-47eb-e4d2-45494519cb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.12      0.50      0.19        70\n",
            "           0       1.00      0.01      0.02       126\n",
            "           1       0.85      0.74      0.79       804\n",
            "\n",
            "    accuracy                           0.63      1000\n",
            "   macro avg       0.65      0.42      0.33      1000\n",
            "weighted avg       0.81      0.63      0.65      1000\n",
            "\n",
            "accuracy: 0.632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC(class_weight={-1:0.66, 0:0.29, 1:0.05})\n",
        "clf.fit(X_train_vect_avg, train_data['stance'])\n",
        "# pipe_tfidf = make_pipeline(word_vectorizer, clf)\n",
        "# pipe_tfidf.fit(train_data_embeddings, train_data['stance'])\n",
        "print_report(clf, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/LinearSVC_Ara2Vec.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egK5dQLFpWmB"
      },
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYAptW2opWmC",
        "outputId": "512b8b28-2267-42c3-b421-17134a1bb295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.36      0.06      0.10        70\n",
            "           0       0.36      0.06      0.11       126\n",
            "           1       0.82      0.98      0.89       804\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.51      0.37      0.37      1000\n",
            "weighted avg       0.73      0.80      0.74      1000\n",
            "\n",
            "accuracy: 0.802\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
        "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = './models/stance/RandomForest_Ara2Vec.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create initial embedding matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkAjiyv18q-n",
        "outputId": "d3d61811-74b2-4af2-bf60-c08023abf629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([12538, 100])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_train_matrix = []\n",
        "for word in vocab:\n",
        "  weights_train_matrix.append(nlp(word).vector)\n",
        "\n",
        "weights_train_matrix = torch.from_numpy(np.array(weights_train_matrix))\n",
        "weights_train_matrix.size()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu7R_QRYpWmK"
      },
      "source": [
        "# ArabicDataset\n",
        "The class that impelements the dataset for arabic tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZZ_dc9dpWmL",
        "outputId": "3197b7ab-8c17-493d-f5b1-82254f19b461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (embedding): Embedding(12538, 100)\n",
            "  (GRU): GRU(100, 100, batch_first=True)\n",
            "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Classifier(weights_train_matrix)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXO2KD-pWmM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Ehn-xNeJpWmM"
      },
      "outputs": [],
      "source": [
        "train_data_tokenized_as_num = train_data_tokenized.apply(lambda x: [word2index[word] for word in x])\n",
        "# apply the same tokenization to the test set\n",
        "test_data_tokenized_as_num = test_data_tokenized.apply(lambda x: [word2index[word] for word in x if word in word2index])\n",
        "train_dataset = ArabicDataset(list(train_data_tokenized_as_num), train_data['stance'] + 1, word2index['<فراغ>'])\n",
        "test_dataset = ArabicDataset(list(test_data_tokenized_as_num), test_data['stance'] + 1, word2index['<فراغ>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IItLwKkpWmN",
        "outputId": "c2688cfc-9911-464b-b577-e824840c966a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:23<00:00,  9.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | Train Loss: 0.1622123467272411         | Train Accuracy: 0.9397538900375366\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_dataset, class_weights= torch.tensor([.5, .4, .1]), epochs=1, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to disk\n",
        "filename = './models/stance/GRU_Ara2Vec.pth'\n",
        "torch.save(model.state_dict(), filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNB-SaT6pWmN"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Z1SWhqpWmO",
        "outputId": "eb0cef84-dc9c-4b6c-a4f5-bfbb6872ad77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 48.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.764\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.33      0.29        70\n",
            "           1       0.37      0.46      0.41       126\n",
            "           2       0.90      0.85      0.88       804\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.51      0.55      0.53      1000\n",
            "weighted avg       0.79      0.76      0.78      1000\n",
            "\n",
            "\n",
            "Test Accuracy: 0.7639999985694885\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 0.41529337,  0.735953  , -1.1096768 ],\n",
              "       [-3.9900215 , -2.0137615 ,  5.4967074 ],\n",
              "       [-4.189743  , -2.1851547 ,  5.6949573 ],\n",
              "       ...,\n",
              "       [-2.2286515 , -1.9069625 ,  3.7751677 ],\n",
              "       [-0.6142367 , -1.37836   ,  1.9775208 ],\n",
              "       [ 0.08092624, -0.49691126,  0.61754024]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "62cd17edec06c1bcb7cce561853235234094d242005d116fab77979ddb024dcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
