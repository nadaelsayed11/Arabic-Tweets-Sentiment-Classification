{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arabic-stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arabicstopwords.arabicstopwords as ast\n",
    "import re\n",
    "import string\n",
    "import qalsadi.lemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from pyarabic.araby import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   category  stance\n",
       "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
       "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
       "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
       "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
       "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
       "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
       "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
       "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
       "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
       "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./DataSet/train.csv',sep=',',header=0)\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = ISRIStemmer()\n",
    "# w= 'حركات'\n",
    "# print(train_data[\"text\"][0])\n",
    "# print(' '.join(st.stem(word) for word in train_data[\"text\"][0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  stance\n",
       "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...         1       1\n",
       "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...         2       1\n",
       "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...         2       1\n",
       "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...         1       1\n",
       "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...         4       0\n",
       "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...         2       0\n",
       "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...         2       1\n",
       "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...         4       0\n",
       "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...         9       0\n",
       "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...         2       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove first row that has the header\n",
    "#train_data.drop(0, axis=0, inplace=True)\n",
    "train_data['category'] = train_data['category'].astype('category').cat.codes\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a stopwords_list\n",
    "stop_words = ast.stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations_list = '''^_-`$%&÷×؛<=>()*&^%][،/;:\"؟.,'{}~¦+|!”…“'''\n",
    "#english_punctuations = string.punctuation\n",
    "#print(english_punctuations)\n",
    "#punctuations_list = arabic_punctuations\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans(punctuations_list, ' '*len(punctuations_list))\n",
    "    return text.translate(translator)\n",
    "    \n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text) # no emoji\n",
    "\n",
    "# a small function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# a small function to remove stop words\n",
    "ArListem = ArabicLightStemmer()\n",
    "def lemmatiz_word(text):\n",
    "    # lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    # return ' '.join(lemmer.lemmatize(word) for word in text.split()) \n",
    "    #---------\n",
    "    #st = ISRIStemmer()\n",
    "    #return ' '.join(st.stem(word) for word in text.split())\n",
    "    return ' '.join(ArListem.light_stem(word) for word in text.split())\n",
    "    \n",
    "def processPost(tweet): \n",
    "\n",
    "    #Remove <LF> from tweet\n",
    "    tweet = re.sub('<LF>', ' ', tweet)\n",
    "    \n",
    "    #Replace @username with empty string\n",
    "    tweet = re.sub('@[^\\s]+', ' ', tweet)\n",
    "    \n",
    "    #remove url\n",
    "    tweet = re.sub('(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})',' ',tweet)\n",
    "    \n",
    "    #remove hashtage #\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "    # remove punctuations\n",
    "    tweet = remove_punctuations(tweet)\n",
    "    \n",
    "    # remove emoji  (not sure with this step)\n",
    "    tweet = remove_emoji(tweet)\n",
    "    \n",
    "    # normalize the tweet\n",
    "    # tweet= normalize_arabic(tweet)\n",
    "    \n",
    "    # remove repeated letters\n",
    "    tweet=re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "\n",
    "    #remove stop words\n",
    "    tweet = remove_stop_words(tweet)\n",
    "\n",
    "    tweet=lemmatiz_word(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية لقاح لعد ما اتابع الاخبار هم بكل مجالاتهم متفوقين وراح يطلع اللقاح قريباً؟<LF>#دعبول_دومه_مسحول\n",
      "دعبول حضر من نت طلب قائد دول إسلام قاح عد تابع اخبار مجال متفوق طلع قاح قريبا دعبول دوم مسحول\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"text\"][9])\n",
    "train_data[\"text\"] = train_data['text'].apply(lambda x: processPost(x))\n",
    "train_data.head(10)\n",
    "print(train_data[\"text\"][9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>أبيض</th>\n",
       "      <th>أتي</th>\n",
       "      <th>أجيل</th>\n",
       "      <th>أحد</th>\n",
       "      <th>أخبار</th>\n",
       "      <th>أخذ</th>\n",
       "      <th>أخر</th>\n",
       "      <th>أخير</th>\n",
       "      <th>أدو</th>\n",
       "      <th>...</th>\n",
       "      <th>يزر</th>\n",
       "      <th>يش</th>\n",
       "      <th>يف</th>\n",
       "      <th>يل</th>\n",
       "      <th>يمن</th>\n",
       "      <th>ين</th>\n",
       "      <th>يه</th>\n",
       "      <th>يوم</th>\n",
       "      <th>يون</th>\n",
       "      <th>يونت</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        أبيض  أتي  أجيل  أحد  أخبار  أخذ  أخر  أخير  أدو  ...  يزر   يش   يف  \\\n",
       "0  1.0   0.0  0.0   0.0  0.0    0.0  0.0  0.0   0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  1.0   0.0  0.0   0.0  0.0    0.0  0.0  0.0   0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  1.0   0.0  0.0   0.0  0.0    0.0  0.0  0.0   0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3  1.0   0.0  0.0   0.0  0.0    0.0  0.0  0.0   0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4  1.0   0.0  0.0   0.0  0.0    0.0  0.0  0.0   0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    يل  يمن   ين   يه  يوم  يون  يونت  \n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "1  0.0  0.0  0.0  0.0  1.0  0.0   0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "4  0.0  0.0  0.0  0.0  1.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),max_features=1000,token_pattern=r\"(?u)\\b[أ-ي]*\\b\")\n",
    "\n",
    "unigramdataGet= word_vectorizer.fit_transform(train_data['text'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "print(len(vocab))\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "pipe_tfidf = make_pipeline(word_vectorizer, clf)\n",
    "pipe_tfidf.fit(train_data['text'], train_data['stance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
    "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
    "rf = RandomForestClassifier()\n",
    "rf_tfidf = rf.fit(X_train_tfidf, train_data['stance'])\n",
    "y_pred = rf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "report = metrics.classification_report(test_data['stance'], y_pred)\n",
    "print(report)\n",
    "print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(test_data['stance'], y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
    "test_data[\"text\"] = test_data['text'].apply(lambda x: processPost(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.45      0.14      0.22        70\n",
      "           0       0.47      0.29      0.36       126\n",
      "           1       0.85      0.95      0.90       804\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.59      0.46      0.49      1000\n",
      "weighted avg       0.77      0.81      0.78      1000\n",
      "\n",
      "accuracy: 0.812\n"
     ]
    }
   ],
   "source": [
    "def print_report(pipe, x_test, y_test):\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    report = metrics.classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print_report(pipe_tfidf, test_data['text'], test_data['stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"text\"]=train_data[\"text\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction \n",
    "1. TF-IDF \n",
    "2. CBoW\n",
    "3. Bag of Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_data_tokenized=train_data[\"text\"].apply(tokenize)\n",
    "w2v_model = Word2Vec(train_data_tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Replace the words in each text message with the learned word vector\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in train_data[\"text\"]])\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in test_data['text']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the word vectors for each sentence (and assign a vector of zeros if the model\n",
    "# did not learn any of the words in the text message during training\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:489: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_store_unique_indices = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:230: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if _joblib.__version__ >= LooseVersion('0.12'):\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.py:158: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:230: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if _joblib.__version__ >= LooseVersion('0.12'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.18      0.03      0.05        70\n",
      "           0       0.25      0.12      0.16       126\n",
      "           1       0.82      0.95      0.88       804\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.42      0.36      0.36      1000\n",
      "weighted avg       0.70      0.78      0.73      1000\n",
      "\n",
      "accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
    "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
    "# y_pred = rf_vect.predict(X_test_vect_avg)\n",
    "# precision = precision_score(test_data['stance'], y_pred)\n",
    "# recall = recall_score(test_data['stance'], y_pred)\n",
    "# accuracy = accuracy_score(test_data['stance'], y_pred)\n",
    "# print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "train_data['stance'].value_counts()\n",
    "\n",
    "# num_of_categories = 450\n",
    "# shuffled = train_data.reindex(np.random.permutation(train_data.index))\n",
    "# e = shuffled[shuffled['stance'] == 1][:num_of_categories]\n",
    "# b = shuffled[shuffled['stance'] == 0][:num_of_categories]\n",
    "# t = shuffled[shuffled['stance'] == -1][:num_of_categories]\n",
    "# concated = pd.concat([e,b,t], ignore_index=True)\n",
    "# #Shuffle the dataset\n",
    "# concated = concated.reindex(np.random.permutation(concated.index))\n",
    "# concated['LABEL'] = 0\n",
    "# labels = to_categorical(concated['LABEL'], num_classes=3)\n",
    "# print(labels[100:110])\n",
    "# if 'stance' in concated.keys():\n",
    "#     concated.drop(['stance'], axis=1)\n",
    "\n",
    "#convert label from one number to vector  of three\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data['stance'])\n",
    "encoded_Y = encoder.transform(train_data['stance'])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder2 = LabelEncoder()\n",
    "encoder2.fit(test_data['stance'])\n",
    "encoded_Y2 = encoder2.transform(test_data['stance'])\n",
    "testt_y = np_utils.to_categorical(encoded_Y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools needed from keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train=train_data[\"text\"]\n",
    "X_test=test_data[\"text\"]\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Use that tokenizer to transform the text messages in the training and test sets\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences so each sequence is the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, 50)\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 32)          407424    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 64)                24832     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 432,451\n",
      "Trainable params: 432,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct a simple RNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
    "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 158s 64ms/step - loss: 0.6419 - accuracy: 0.7896 - val_loss: 0.5433 - val_accuracy: 0.7820\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.4678 - accuracy: 0.8127 - val_loss: 0.5280 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 9s 42ms/step - loss: 0.3800 - accuracy: 0.8454 - val_loss: 0.5155 - val_accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 9s 42ms/step - loss: 0.3265 - accuracy: 0.8675 - val_loss: 0.5819 - val_accuracy: 0.8120\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 0.2799 - accuracy: 0.8892 - val_loss: 0.6383 - val_accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 10s 48ms/step - loss: 0.2340 - accuracy: 0.9103 - val_loss: 0.6911 - val_accuracy: 0.8040\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.2059 - accuracy: 0.9219 - val_loss: 0.6792 - val_accuracy: 0.8060\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.1746 - accuracy: 0.9370 - val_loss: 0.6682 - val_accuracy: 0.7860\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 0.1548 - accuracy: 0.9423 - val_loss: 0.7129 - val_accuracy: 0.7920\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.1338 - accuracy: 0.9516 - val_loss: 0.7061 - val_accuracy: 0.7920\n"
     ]
    }
   ],
   "source": [
    "# Fit the RNN model\n",
    "#y_train=train_data[\"stance\"].values\n",
    "#y_test=test_data[\"stance\"].values\n",
    "history = model.fit(X_train_seq_padded, dummy_y, \n",
    "                    batch_size=32, epochs=10,\n",
    "                    validation_data=(X_test_seq_padded[:500], testt_y[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-34c9480ff55c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_test_seq_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestt_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f} \\n  f1_scor: {:0.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#x = model.predict(X_test_seq_padded[500:])\n",
    "#print(x)\n",
    "accr = model.evaluate( X_test_seq_padded[500:], testt_y[500:],verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f} \\n  f1_scor: {:0.3f}'.format(accr[0],accr[1],accr[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a4ee8fc533cc884599b71a8fec4c5f6888a620674ef2e26743516198ba2fa29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
