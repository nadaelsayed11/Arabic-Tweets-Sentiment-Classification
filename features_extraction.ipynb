{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arabic-stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import qalsadi.lemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from pyarabic.araby import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from pre_processing_post import processPost\n",
    "from extract_features import get_unigram_features, get_word_embedding_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed functions\n",
    "def print_report(pipe, x_test, y_test):\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    report = metrics.classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   category  stance\n",
       "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
       "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
       "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
       "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
       "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
       "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
       "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
       "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
       "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
       "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./DataSet/train.csv',sep=',',header=0)\n",
    "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  stance\n",
       "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...         1       1\n",
       "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...         2       1\n",
       "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...         2       1\n",
       "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...         1       1\n",
       "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...         4       0\n",
       "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...         2       0\n",
       "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...         2       1\n",
       "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...         4       0\n",
       "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...         9       0\n",
       "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...         2       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove first row that has the header\n",
    "train_data['category'] = train_data['category'].astype('category').cat.codes\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3741713d26a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "train_data=train_data.drop('category',axis=1)\n",
    "y=train_data['stance']\n",
    "print(Counter(train_data['stance']))\n",
    "train_data=train_data.drop('stance',axis=1)\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(random_state=3)\n",
    "# fit and apply the transform\n",
    "train_data[\"text\"], train_data['stance'] = oversample.fit_resample(train_data, y)\n",
    "print(Counter(train_data['stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية لقاح لعد ما اتابع الاخبار هم بكل مجالاتهم متفوقين وراح يطلع اللقاح قريباً؟<LF>#دعبول_دومه_مسحول\n",
      "دعبول حضر من نت طلب قائد دول إسلام قاح عد تابع اخبار مجال متفوق طلع قاح قريبا دعبول دوم مسحول\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"text\"][9])\n",
    "train_data[\"text\"] = train_data['text'].apply(lambda x: processPost(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))\n",
    "print(train_data[\"text\"][9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  990  991  992  993  \\\n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
       "1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigramdata_features, word_vectorizer, vocab = get_unigram_features(train_data)\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.16      0.25        70\n",
      "           0       0.49      0.33      0.39       126\n",
      "           1       0.85      0.95      0.90       804\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.64      0.48      0.51      1000\n",
      "weighted avg       0.79      0.82      0.79      1000\n",
      "\n",
      "accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "pipe_tfidf = make_pipeline(word_vectorizer, clf)\n",
    "pipe_tfidf.fit(train_data['text'], train_data['stance'])\n",
    "print_report(pipe_tfidf, test_data['text'], test_data['stance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.16      0.24        70\n",
      "           0       0.57      0.25      0.34       126\n",
      "           1       0.84      0.97      0.90       804\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.64      0.46      0.50      1000\n",
      "weighted avg       0.79      0.82      0.79      1000\n",
      "\n",
      "accuracy: 0.821\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
    "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
    "rf = RandomForestClassifier()\n",
    "rf_tfidf = rf.fit(X_train_tfidf, train_data['stance'])\n",
    "y_pred = rf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print_report(rf_tfidf, X_test_tfidf, test_data['stance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CMP\\level4\\First\\NLP\\Arabic-Tweets-Sentiment-Classification\\extract_features.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for ls in train_data[\"text\"]])\n",
      "e:\\CMP\\level4\\First\\NLP\\Arabic-Tweets-Sentiment-Classification\\extract_features.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for ls in test_data['text']])\n"
     ]
    }
   ],
   "source": [
    "X_train_vect_avg, X_test_vect_avg = get_word_embedding_features(train_data, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Raghod\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        70\n",
      "           0       0.00      0.00      0.00       126\n",
      "           1       0.80      1.00      0.89       804\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.27      0.33      0.30      1000\n",
      "weighted avg       0.65      0.80      0.72      1000\n",
      "\n",
      "accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train_vect_avg, train_data['stance'])\n",
    "print_report(clf, X_test_vect_avg, test_data['stance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.04      0.08        70\n",
      "           0       0.42      0.10      0.17       126\n",
      "           1       0.82      0.98      0.89       804\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.66      0.37      0.38      1000\n",
      "weighted avg       0.76      0.80      0.74      1000\n",
      "\n",
      "accuracy: 0.803\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
    "print_report(rf_vect, X_test_vect_avg, test_data['stance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERDataset\n",
    "The class that impelements the dataset for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    ##################### TODO: create two tensors one for x and the other for labels ###############################\n",
    "    list_len = [len(i) for i in x]\n",
    "    MAX_LENGTH = max(list_len) \n",
    "    for i in range(len(x)):\n",
    "      x[i] = np.pad(x[i], (0, MAX_LENGTH-len(x[i])), 'constant', constant_values=(pad))\n",
    "      y[i] = np.pad(y[i], (0, MAX_LENGTH-len(y[i])), 'constant', constant_values=(0))\n",
    "\n",
    "    self.x = torch.from_numpy(np.array(x)) \n",
    "    self.y = torch.from_numpy(np.array(y))\n",
    "\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "    return self.x.shape[0]\n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return (self.x[idx], self.y[idx])\n",
    "    ##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6b8eb6bad1c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \"\"\"\n\u001b[0;32m      4\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mour\u001b[0m \u001b[0mNER\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mInputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6b8eb6bad1c3>\u001b[0m in \u001b[0;36mNER\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \"\"\"\n\u001b[0;32m      4\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mour\u001b[0m \u001b[0mNER\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mInputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "class NER(nn.Module):\n",
    "  def __init__(self, vocab_size=len(vocab), embedding_dim=50, hidden_size=50, n_classes=3):\n",
    "    \"\"\"\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    self.hidden_size = hidden_size\n",
    "    super(NER, self).__init__()\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    # (3) Create a linear layer with number of neorons = n_classes\n",
    "    self.linear = nn.Linear(hidden_size, n_classes)\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    final_output = None\n",
    "    ######################### TODO: implement the forward pass ####################################\n",
    "    final_output, _ = self.lstm(self.embedding(sentences))\n",
    "    final_output = self.linear(final_output)\n",
    "    final_output = final_output[:, -1, :]\n",
    "    ###############################################################################################\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER(\n",
      "  (embedding): Embedding(1000, 50)\n",
      "  (lstm): LSTM(50, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NER()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\features_extraction.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39mstance\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# num_of_categories = 450\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# shuffled = train_data.reindex(np.random.permutation(train_data.index))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# e = shuffled[shuffled['stance'] == 1][:num_of_categories]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/features_extraction.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#convert label from one number to vector  of three\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "train_data['stance'].value_counts()\n",
    "\n",
    "# num_of_categories = 450\n",
    "# shuffled = train_data.reindex(np.random.permutation(train_data.index))\n",
    "# e = shuffled[shuffled['stance'] == 1][:num_of_categories]\n",
    "# b = shuffled[shuffled['stance'] == 0][:num_of_categories]\n",
    "# t = shuffled[shuffled['stance'] == -1][:num_of_categories]\n",
    "# concated = pd.concat([e,b,t], ignore_index=True)\n",
    "# #Shuffle the dataset\n",
    "# concated = concated.reindex(np.random.permutation(concated.index))\n",
    "# concated['LABEL'] = 0\n",
    "# labels = to_categorical(concated['LABEL'], num_classes=3)\n",
    "# print(labels[100:110])\n",
    "# if 'stance' in concated.keys():\n",
    "#     concated.drop(['stance'], axis=1)\n",
    "\n",
    "#convert label from one number to vector  of three\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data['stance'])\n",
    "encoded_Y = encoder.transform(train_data['stance'])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder2 = LabelEncoder()\n",
    "encoder2.fit(test_data['stance'])\n",
    "encoded_Y2 = encoder2.transform(test_data['stance'])\n",
    "testt_y = np_utils.to_categorical(encoded_Y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools needed from keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train=train_data[\"text\"]\n",
    "X_test=test_data[\"text\"]\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Use that tokenizer to transform the text messages in the training and test sets\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences so each sequence is the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, 50)\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 32)          407424    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 64)                24832     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 432,451\n",
      "Trainable params: 432,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct a simple RNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
    "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 158s 64ms/step - loss: 0.6419 - accuracy: 0.7896 - val_loss: 0.5433 - val_accuracy: 0.7820\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.4678 - accuracy: 0.8127 - val_loss: 0.5280 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 9s 42ms/step - loss: 0.3800 - accuracy: 0.8454 - val_loss: 0.5155 - val_accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 9s 42ms/step - loss: 0.3265 - accuracy: 0.8675 - val_loss: 0.5819 - val_accuracy: 0.8120\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 0.2799 - accuracy: 0.8892 - val_loss: 0.6383 - val_accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 10s 48ms/step - loss: 0.2340 - accuracy: 0.9103 - val_loss: 0.6911 - val_accuracy: 0.8040\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.2059 - accuracy: 0.9219 - val_loss: 0.6792 - val_accuracy: 0.8060\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.1746 - accuracy: 0.9370 - val_loss: 0.6682 - val_accuracy: 0.7860\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 0.1548 - accuracy: 0.9423 - val_loss: 0.7129 - val_accuracy: 0.7920\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.1338 - accuracy: 0.9516 - val_loss: 0.7061 - val_accuracy: 0.7920\n"
     ]
    }
   ],
   "source": [
    "# Fit the RNN model\n",
    "#y_train=train_data[\"stance\"].values\n",
    "#y_test=test_data[\"stance\"].values\n",
    "history = model.fit(X_train_seq_padded, dummy_y, \n",
    "                    batch_size=32, epochs=10,\n",
    "                    validation_data=(X_test_seq_padded[:500], testt_y[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-34c9480ff55c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_test_seq_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestt_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f} \\n  f1_scor: {:0.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#x = model.predict(X_test_seq_padded[500:])\n",
    "#print(x)\n",
    "accr = model.evaluate( X_test_seq_padded[500:], testt_y[500:],verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f} \\n  f1_scor: {:0.3f}'.format(accr[0],accr[1],accr[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a4ee8fc533cc884599b71a8fec4c5f6888a620674ef2e26743516198ba2fa29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
