{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "294CcummpWl2",
        "outputId": "b6146a3d-18b7-4510-ab60-2bb5d8e82f2b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32me:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\model_building.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/model_building.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/model_building.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR8uhavw73yb",
        "outputId": "d483f37b-64a5-4dda-8d4a-5e50dd066b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nlp_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/nlp_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U139J06L2VSB",
        "outputId": "f678c9c5-9f66-471f-aca6-3abc474a0923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic-stopwords in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from arabic-stopwords) (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic>=0.6.2->arabic-stopwords) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: qalsadi in /usr/local/lib/python3.8/dist-packages (0.4.5)\n",
            "Requirement already satisfied: naftawayh>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: Arabic-Stopwords>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: alyahmor>=0.1 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.1.5)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.16.0)\n",
            "Requirement already satisfied: pickledb>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.9.2)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.6.15)\n",
            "Requirement already satisfied: arramooz-pysqlite>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: tashaphyne>=0.3.4.1 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.15.0)\n",
            "Requirement already satisfied: libqutrub>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install arabic-stopwords\n",
        "!pip install qalsadi\n",
        "!pip install pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giRMTMSppgrI",
        "outputId": "1e5469a5-198b-49d3-ee15-ba7a3d0fde3e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string\n",
        "import qalsadi.lemmatizer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from pyarabic.araby import tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "from gru_model import ArabicDataset, Classifier, evaluate, train\n",
        "from pre_processing_post import processPost\n",
        "from feature_extraction import get_ngram_features, get_word_embedding_features, avg_word_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8V_Ralq6pWl4"
      },
      "outputs": [],
      "source": [
        "# needed functions\n",
        "def print_report(pipe, x_test, y_test):\n",
        "    y_pred = pipe.predict(x_test)\n",
        "    report = metrics.classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EsnK9zupWl5"
      },
      "source": [
        "# Read train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "dlD_8DNapWl6",
        "outputId": "5c801c19-c001-4554-88ba-8c560c4914ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('./DataSet/train.csv',sep=',',header=0)\n",
        "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "a-m343EOpWl8",
        "outputId": "a8d192b6-009a-499f-8545-bc1903d0ca9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...         1       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...         2       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...         2       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...         1       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...         4       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...         2       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...         2       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...         4       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...         9       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...         2       1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#remove first row that has the header\n",
        "train_data['category'] = train_data['category'].astype('category').cat.codes\n",
        "train_data.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Po_uSrO-T6sS"
      },
      "source": [
        "## Over Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jf6OGm-JNNAm"
      },
      "outputs": [],
      "source": [
        "# !pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AMwOtKGOpWl8"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "# train_data=train_data.drop('category',axis=1)\n",
        "# y=train_data['stance']\n",
        "# print(Counter(train_data['stance']))\n",
        "# train_data=train_data.drop('stance',axis=1)\n",
        "# # define oversampling strategy\n",
        "# oversample = RandomOverSampler(random_state=3)\n",
        "# # fit and apply the transform\n",
        "# train_data[\"text\"], train_data['stance'] = oversample.fit_resample(train_data, y)\n",
        "# print(Counter(train_data['stance']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3SV1t53pWl9"
      },
      "source": [
        "# Pre-Processing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWbuBoDgpWl9",
        "outputId": "dfe81e23-b7d5-4c71-a7a7-6202ef2aa394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية لقاح لعد ما اتابع الاخبار هم بكل مجالاتهم متفوقين وراح يطلع اللقاح قريباً؟<LF>#دعبول_دومه_مسحول\n",
            "دعبول حضر من نت طلب قائد دول إسلام قاح عد تابع اخبار مجال متفوق طلع قاح قريبا دعبول دوم مسحول\n"
          ]
        }
      ],
      "source": [
        "print(train_data[\"text\"][9])\n",
        "train_data[\"text\"] = train_data['text'].apply(lambda x: processPost(x))\n",
        "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))\n",
        "print(train_data[\"text\"][9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f26urbLgkiKi"
      },
      "source": [
        "## Ara2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EfJCUEzEkpA-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'ar_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# load AraVec Spacy model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PqSckdSVkvrh"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing Class\n",
        "class Preprocessor:\n",
        "    def __init__(self, tokenizer, **cfg):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, text):\n",
        "        # preprocessed = processPost(text)\n",
        "        return self.tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cUqO4-OFk3Ch"
      },
      "outputs": [],
      "source": [
        "# Apply the `Preprocessor` Class\n",
        "nlp.tokenizer = Preprocessor(nlp.tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c7WB_vA_sIO"
      },
      "source": [
        "## create vocablary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RcoWDCqQ_w5n"
      },
      "outputs": [],
      "source": [
        "train_data_tokenized = train_data['text'].apply(tokenize)\n",
        "test_data_tokenized = test_data['text'].apply(tokenize)\n",
        "#merge all the sentences in one list\n",
        "vocab = [item for sublist in train_data_tokenized for item in sublist]\n",
        "vocab = list(set(vocab))\n",
        "vocab.append('<فراغ>')\n",
        "vocab.insert(0, '<مجهول>')\n",
        "word2index = {word: i for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<مجهول>',\n",
              " 'غذائ',\n",
              " 'مخنوق',\n",
              " 'محيط',\n",
              " 'مطر',\n",
              " 'خسار',\n",
              " 'بريطاني',\n",
              " 'ميغيل',\n",
              " 'DuckDuckGo',\n",
              " 'قراب',\n",
              " 'جانفي',\n",
              " 'يأس',\n",
              " 'جزع',\n",
              " 'تعرف',\n",
              " 'خي',\n",
              " 'وليد',\n",
              " 'ضعيف',\n",
              " '٦٠',\n",
              " 'متخليش',\n",
              " 'حاخام',\n",
              " 'جدآ',\n",
              " 'تصريح',\n",
              " 'حكو',\n",
              " 'حديد',\n",
              " 'مرتفع',\n",
              " 'لبانوراما',\n",
              " '96',\n",
              " 'تحف',\n",
              " 'شكش',\n",
              " 'مضادلمرض',\n",
              " 'عجبا',\n",
              " 'مامرت',\n",
              " 'وبار',\n",
              " 'هلأتن',\n",
              " 'حدوث',\n",
              " 'صنعو',\n",
              " 'ختف',\n",
              " 'سمعوا',\n",
              " 'بقا',\n",
              " 'و3',\n",
              " 'وش',\n",
              " 'تمام',\n",
              " 'إدار',\n",
              " 'اردن',\n",
              " 'حبيبه',\n",
              " 'تجا',\n",
              " 'وظيفه',\n",
              " 'معترف',\n",
              " '٪🤲🤚',\n",
              " 'يفيق',\n",
              " 'موروث',\n",
              " 'سيطره',\n",
              " 'يام',\n",
              " 'امرأ',\n",
              " 'لاستغلاله',\n",
              " 'حث',\n",
              " 'عشربن',\n",
              " '141هج',\n",
              " '75',\n",
              " 'مضطر',\n",
              " 'مرتش',\n",
              " 'بخار',\n",
              " '٪\\u061c\\u200d♀️',\n",
              " 'فحوص',\n",
              " 'إحساس',\n",
              " '17',\n",
              " 'قطع',\n",
              " '81',\n",
              " 'امتحان',\n",
              " 'جس',\n",
              " 'عنجد',\n",
              " 'إجهاد',\n",
              " 'متشد',\n",
              " 'مهدود',\n",
              " 'قدمت',\n",
              " 'الفرنسيه73',\n",
              " 'يدوا',\n",
              " 'ملتيميديا',\n",
              " 'مركزيا',\n",
              " 'تعويل',\n",
              " 'غيبوب',\n",
              " 'كلاسيك',\n",
              " 'يستهين',\n",
              " 'كي',\n",
              " 'اشخاص',\n",
              " 'طفئو',\n",
              " 'حواش',\n",
              " 'عميد',\n",
              " 'الأولويةتغطية',\n",
              " 'شورى',\n",
              " 'وتر',\n",
              " 'يحميكش',\n",
              " 'وثق',\n",
              " 'زرنگ',\n",
              " 'إطاح',\n",
              " 'يسير',\n",
              " '26يناير',\n",
              " 'خاف',\n",
              " 'غداد',\n",
              " 'صغار',\n",
              " 'تيباز',\n",
              " 'إكمال',\n",
              " 'شكي',\n",
              " 'حم',\n",
              " 'ناقش',\n",
              " 'معتاد',\n",
              " '18دولار',\n",
              " 'CORAVAX',\n",
              " 'افيونواتايمين',\n",
              " 'مجيد',\n",
              " '٥٠الف',\n",
              " 'ديكلار',\n",
              " 'طالع',\n",
              " 'حجبت',\n",
              " 'طغى',\n",
              " 'اقلام',\n",
              " 'رزاق',\n",
              " '94',\n",
              " 'قب',\n",
              " 'بسو',\n",
              " 'تضر',\n",
              " 'عافية',\n",
              " 'ميانمار',\n",
              " 'زر',\n",
              " 'يستهضم',\n",
              " 'سعيد',\n",
              " 'إفتاح',\n",
              " 'البرازيل',\n",
              " 'صمت',\n",
              " 'تاخرت',\n",
              " 'احثا',\n",
              " 'معلوم',\n",
              " 'تنمر',\n",
              " 'اختشو',\n",
              " 'يونسيف',\n",
              " 'متلون',\n",
              " 'تاريخية',\n",
              " 'إشراف',\n",
              " 'اكلينيك',\n",
              " 'ديش',\n",
              " 'راع',\n",
              " 'فكير',\n",
              " 'هزل',\n",
              " 'مرزوق',\n",
              " 'مليان',\n",
              " 'هالحقن',\n",
              " 'خمين',\n",
              " 'عباء',\n",
              " 'صلو',\n",
              " '1936',\n",
              " 'سياح',\n",
              " 'ايحمد',\n",
              " 'ـ6',\n",
              " 'وباءالكورونا',\n",
              " 'جواب',\n",
              " 'انست',\n",
              " 'دمج',\n",
              " 'وفير',\n",
              " 'جهال',\n",
              " 'دهاش',\n",
              " 'قليوب',\n",
              " '️',\n",
              " 'مستغرب',\n",
              " '⛔️',\n",
              " 'لحاء',\n",
              " '٤٥',\n",
              " 'راير',\n",
              " 'انكليز',\n",
              " 'راؤول',\n",
              " 'ضبع',\n",
              " '26يناير²0²¹',\n",
              " 'يدفش',\n",
              " 'عون',\n",
              " 'آتي',\n",
              " 'افتاح',\n",
              " 'ستطاع',\n",
              " 'علميه',\n",
              " 'كلى',\n",
              " 'علبال',\n",
              " 'توصيا',\n",
              " 'وجد',\n",
              " 'حتو',\n",
              " 'احقا',\n",
              " 'حلت',\n",
              " 'شتد',\n",
              " 'ثاب',\n",
              " 'قدام',\n",
              " 'ميام',\n",
              " '❎',\n",
              " 'دعاء',\n",
              " 'اشتراك',\n",
              " '٣٧',\n",
              " 'ين7',\n",
              " 'ماعد',\n",
              " 'ضخم',\n",
              " 'معلق',\n",
              " 'وتطمنالتي',\n",
              " 'خراج',\n",
              " 'صحصح',\n",
              " 'هالسلط',\n",
              " 'جديد',\n",
              " 'هيج',\n",
              " 'ارزا',\n",
              " 'з',\n",
              " 'مشيئ',\n",
              " 'يلم',\n",
              " 'مت',\n",
              " 'طائفيه',\n",
              " 'س',\n",
              " 'مستقل',\n",
              " 'esential',\n",
              " 'خول',\n",
              " 'حوار',\n",
              " 'فوروم',\n",
              " 'واخر',\n",
              " 'ستبق',\n",
              " 'مدينه',\n",
              " 'ميلزموش',\n",
              " 'توسع',\n",
              " '2025',\n",
              " 'ظير',\n",
              " 'اخونج',\n",
              " 'مدير',\n",
              " 'غبر',\n",
              " 'شهاد',\n",
              " 'غالبا',\n",
              " 'طاىق',\n",
              " '🦋',\n",
              " 'غلى',\n",
              " 'بوس',\n",
              " 'لتحصين4إلى5ملاين',\n",
              " 'أمـير',\n",
              " 'رنسيس',\n",
              " 'تعويض',\n",
              " 'مسلمين',\n",
              " 'أشع',\n",
              " 'شرق',\n",
              " 'تالت',\n",
              " 'مستمر',\n",
              " 'سرويجاي',\n",
              " 'تميم',\n",
              " 'نماذج',\n",
              " 'يجاد',\n",
              " 'لقح',\n",
              " 'صواب',\n",
              " 'فتن',\n",
              " 'side',\n",
              " 'مخط',\n",
              " 'مولى',\n",
              " 'هرطق',\n",
              " 'مقبر',\n",
              " 'عاقد',\n",
              " 'ضامن',\n",
              " 'يعطو',\n",
              " 'تضح',\n",
              " 'فاصل',\n",
              " 'Xinhua',\n",
              " 'مقاسم',\n",
              " 'photoshot',\n",
              " 'فتح',\n",
              " 'اشاع',\n",
              " 'شعور',\n",
              " '6',\n",
              " 'مجموع',\n",
              " 'ذكرو',\n",
              " 'of',\n",
              " 'فضاء',\n",
              " 'مرسومـ',\n",
              " 'مستدام',\n",
              " 'سحاب',\n",
              " 'براء',\n",
              " 'ردود',\n",
              " 'وسايتوفارم',\n",
              " 'مايك',\n",
              " 'ويت',\n",
              " 'اگول',\n",
              " 'يزابيث',\n",
              " 'ولائ',\n",
              " 'ايوجد',\n",
              " 'استودع',\n",
              " 'تصدر',\n",
              " 'موصول',\n",
              " 'إمكاني',\n",
              " 'زعل',\n",
              " 'إمانه',\n",
              " 'متفائلا',\n",
              " 'ملكه',\n",
              " 'إصرار',\n",
              " 'تلق',\n",
              " 'شتاء',\n",
              " 'منضرب',\n",
              " 'سخاف',\n",
              " 'ركى',\n",
              " 'رضى',\n",
              " 'متمول',\n",
              " 'EN',\n",
              " 'سفيه',\n",
              " 'فانيل',\n",
              " 'تصنع',\n",
              " 'غاليني',\n",
              " 'زياء',\n",
              " 'ركيب',\n",
              " 'يلوس',\n",
              " 'دخيل',\n",
              " 'مضاده',\n",
              " 'حرفيا',\n",
              " 'اتاح',\n",
              " 'لجيكا',\n",
              " 'فيز',\n",
              " 'استحواذ',\n",
              " 'اهذرم',\n",
              " 'عصمل',\n",
              " 'الفرنسيه78',\n",
              " 'صميم',\n",
              " 'آمنا',\n",
              " 'حرنو',\n",
              " 'تسعبل',\n",
              " 'إذاعـ',\n",
              " 'عاصم',\n",
              " 'عنت',\n",
              " 'حليف',\n",
              " 'أخو',\n",
              " 'فرج',\n",
              " 'مستوطن',\n",
              " 'تخلص',\n",
              " 'زغار',\n",
              " 'يحفظك',\n",
              " 'فاقم',\n",
              " 'قتراب',\n",
              " 'Amazon',\n",
              " 'عاطف',\n",
              " 'فص',\n",
              " 'بانتظار',\n",
              " 'رزق',\n",
              " 'هيلر',\n",
              " 'اعضاء',\n",
              " 'عار',\n",
              " 'قراط',\n",
              " 'جدوا',\n",
              " 'اطل',\n",
              " 'موبيلا',\n",
              " 'مدرع',\n",
              " 'تنفيذي',\n",
              " 'عليك',\n",
              " 'مقصر',\n",
              " 'ماون',\n",
              " 'استعمال',\n",
              " 'بنا',\n",
              " 'مرفق',\n",
              " 'ملهوف',\n",
              " 'COROVAX',\n",
              " 'يدوق',\n",
              " 'اخرى',\n",
              " '١٠',\n",
              " 'بتكر',\n",
              " 'دروز',\n",
              " 'ستحضر',\n",
              " 'وسايط',\n",
              " 'ستمع',\n",
              " 'ناكر',\n",
              " 'متفوق',\n",
              " 'زاد',\n",
              " 'مكمل',\n",
              " 'حمدله',\n",
              " 'ينغث',\n",
              " 'موناليز',\n",
              " 'إيخيلوف',\n",
              " 'درج',\n",
              " 'سرت',\n",
              " 'سياسيه',\n",
              " 'مز',\n",
              " 'نفى',\n",
              " 'رعا',\n",
              " 'معقم',\n",
              " 'ماشهد',\n",
              " 'عسكريا',\n",
              " 'كل',\n",
              " 'منافسه',\n",
              " 'بوابه',\n",
              " 'BNT162b2',\n",
              " 'مخترق',\n",
              " 'ف',\n",
              " 'عكف',\n",
              " 'سخص',\n",
              " 'تناقل',\n",
              " 'تواز',\n",
              " 'جيبو',\n",
              " 'نرويجيا',\n",
              " 'طفل',\n",
              " 'يكف',\n",
              " 'يعرفو',\n",
              " 'مناطق',\n",
              " 'إنترفاكس',\n",
              " 'ذرع',\n",
              " 'أفاع',\n",
              " 'تفسير',\n",
              " 'غياب',\n",
              " 'جزءالذ',\n",
              " 'تدج',\n",
              " 'ـورار',\n",
              " 'دمشق',\n",
              " 'حاب',\n",
              " 'ྀ•̤̣̈̇♡̬̩̃•̤̣̈̇❀❥♡',\n",
              " 'عظيما',\n",
              " 'جسام',\n",
              " '￼',\n",
              " 'رافعا',\n",
              " 'شجار',\n",
              " 'ختلاق',\n",
              " 'علمائ',\n",
              " 'حمام',\n",
              " 'اهداف',\n",
              " 'ندقيق',\n",
              " 'سيرنج',\n",
              " 'فاد',\n",
              " 'تحقيقا',\n",
              " 'EGYPT',\n",
              " 'احلام',\n",
              " 'أمريك',\n",
              " 'امبارح',\n",
              " 'تصلب',\n",
              " 'جائحة',\n",
              " 'ملحق',\n",
              " 'حساف',\n",
              " 'وكلت',\n",
              " 'نهضر',\n",
              " 'رثاء',\n",
              " 'شهير',\n",
              " 'مديح',\n",
              " 'هالfetish',\n",
              " 'لاس',\n",
              " 'جشع',\n",
              " 'عائد',\n",
              " 'ماينتشر',\n",
              " 'ريج',\n",
              " 'آ',\n",
              " 'زمن',\n",
              " 'ذ',\n",
              " 'ياساد',\n",
              " 'ياخدو',\n",
              " 'بوم',\n",
              " 'خاطر',\n",
              " 'نستكمل',\n",
              " 'عوف',\n",
              " 'asian',\n",
              " 'قان',\n",
              " 'ياما',\n",
              " 'ستناد',\n",
              " 'اسبوع',\n",
              " 'غيث',\n",
              " 'رجم',\n",
              " 'شرس',\n",
              " 'مصادر',\n",
              " 'منظوم',\n",
              " 'غاز',\n",
              " 'مور',\n",
              " 'قبض',\n",
              " 'منصير',\n",
              " 'اعتزاز',\n",
              " 'إكسبرس',\n",
              " 'عمى',\n",
              " 'طرابلس',\n",
              " 'تنويه',\n",
              " 'مرأ',\n",
              " 'قرى',\n",
              " 'قاليم',\n",
              " 'سكرتير',\n",
              " 'ميترايوز',\n",
              " 'تصميم',\n",
              " 'تامي',\n",
              " '١٩لتعرف',\n",
              " 'تناكح',\n",
              " 'ذين',\n",
              " 'صايب',\n",
              " 'مرعى',\n",
              " 'me',\n",
              " 'جود',\n",
              " 'سعوديه',\n",
              " 'حامد',\n",
              " 'إنفجار',\n",
              " 'رنا',\n",
              " 'كول',\n",
              " 'مشرف',\n",
              " 'إختار',\n",
              " 'واگه',\n",
              " 'قانعي',\n",
              " 'ارق',\n",
              " 'ابليس',\n",
              " 'طعمنا',\n",
              " 'عرض',\n",
              " 'اطم',\n",
              " 'ابداع',\n",
              " 'نفصل',\n",
              " 'إطلع',\n",
              " 'نمساو',\n",
              " '69',\n",
              " 'ول',\n",
              " 'تنتقل',\n",
              " 'احتراز',\n",
              " 'طبائ',\n",
              " 'رؤي',\n",
              " '‼️',\n",
              " 'مشرك',\n",
              " 'حتصير',\n",
              " 'تخترع',\n",
              " 'زميل',\n",
              " 'تحميل',\n",
              " '371',\n",
              " 'احدة',\n",
              " 'رئيسى',\n",
              " 'إختيار',\n",
              " 'يعانوا',\n",
              " 'استقدام',\n",
              " 'واضح',\n",
              " 'ظري',\n",
              " 'حبيب',\n",
              " 'رواتب',\n",
              " 'نه',\n",
              " 'يقظ',\n",
              " 'يوثق',\n",
              " 'جاع',\n",
              " 'يك',\n",
              " 'وصل',\n",
              " 'فحص',\n",
              " 'متجاوز',\n",
              " 'yi',\n",
              " 'منكر',\n",
              " 'توازن',\n",
              " 'مربط',\n",
              " 'فتخار',\n",
              " 'غي',\n",
              " 'عشا',\n",
              " 'شرائح',\n",
              " 'مواقع',\n",
              " 'زهر',\n",
              " 'بني',\n",
              " 'شمس',\n",
              " 'جنود',\n",
              " 'احتراف',\n",
              " 'قحنا',\n",
              " 'ماصر',\n",
              " 'لجيك',\n",
              " 'جلجوء',\n",
              " 'مرد',\n",
              " 'همي',\n",
              " 'ارتيل',\n",
              " 'رهاب',\n",
              " 'صرع',\n",
              " 'أيام',\n",
              " 'ملجأ',\n",
              " 'تجربه',\n",
              " 'مستر',\n",
              " 'SAUDI',\n",
              " 'صحفي',\n",
              " '١٥٠',\n",
              " 'مداخيل',\n",
              " 'هذالقاح',\n",
              " 'كفير',\n",
              " 'شفاء',\n",
              " 'NFL',\n",
              " '❣️❣️',\n",
              " 'ياكل',\n",
              " 'عشرين',\n",
              " 'سينوفارم',\n",
              " 'شوارزنيجر',\n",
              " 'فايسبوك',\n",
              " 'ضل',\n",
              " 'شأ',\n",
              " 'ࣖ',\n",
              " 'محظور',\n",
              " 'يموتوا',\n",
              " 'عملوا',\n",
              " 'وفيت',\n",
              " 'معتقل',\n",
              " '2020',\n",
              " 'خلد',\n",
              " 'مسلوب',\n",
              " 'فودك',\n",
              " 'نصيب',\n",
              " 'امن',\n",
              " 'استباق',\n",
              " 'انهاء',\n",
              " 'جاس',\n",
              " 'استرخاص',\n",
              " 'جحيم',\n",
              " 'هاكرز',\n",
              " 'منتبه',\n",
              " 'هيدخل',\n",
              " 'مارتى',\n",
              " 'مؤون',\n",
              " 'World',\n",
              " 'كتاب',\n",
              " 'وهم',\n",
              " 'مرئ',\n",
              " 'خراف',\n",
              " 'غب',\n",
              " 'مضارب',\n",
              " 'كرس',\n",
              " 'حليل',\n",
              " 'دراس',\n",
              " 'توزع',\n",
              " '➡️',\n",
              " 'خوي',\n",
              " 'انتهائ',\n",
              " 'مدبوح',\n",
              " 'ايروس',\n",
              " 'عجلا',\n",
              " 'جاكيت',\n",
              " 'هيتكفل',\n",
              " 'هياخد',\n",
              " 'عوض',\n",
              " 'حيبلش',\n",
              " 'جورج',\n",
              " 'گدر',\n",
              " 'على',\n",
              " 'سينم',\n",
              " 'منتج',\n",
              " 'ياسم',\n",
              " 'تعرفوا',\n",
              " 'فلوس',\n",
              " 'سينافورم',\n",
              " '2019',\n",
              " 'تنميةو',\n",
              " 'علی',\n",
              " 'الخطر',\n",
              " 'عميتغنج',\n",
              " 'محظوظ',\n",
              " 'يشف',\n",
              " 'جلفار',\n",
              " 'إذاع',\n",
              " 'شربل',\n",
              " 'قاش',\n",
              " 'إنتاجھ',\n",
              " 'يضر',\n",
              " 'استخدم',\n",
              " 'جعب',\n",
              " 'Maroc',\n",
              " 'لاه',\n",
              " 'افريق',\n",
              " 'آصحاب',\n",
              " 'ـ١٦',\n",
              " 'شجير',\n",
              " 'ملاه',\n",
              " 'منصف',\n",
              " 'أحباب',\n",
              " 'امار',\n",
              " 'علاء',\n",
              " 'تقد',\n",
              " 'إتخاذ',\n",
              " 'مندوب',\n",
              " 'ورث',\n",
              " 'فائده',\n",
              " 'مسعا',\n",
              " 'antibodies',\n",
              " 'أنفاس',\n",
              " 'إله',\n",
              " 'قدر',\n",
              " 'تعبان',\n",
              " '١٣٠',\n",
              " 'أثير',\n",
              " 'ئق',\n",
              " 'عتبر',\n",
              " 'سندباد',\n",
              " 'يابي',\n",
              " 'AfordableVacine4Al',\n",
              " 'جنون',\n",
              " 'مافي',\n",
              " '٤٥٠',\n",
              " 'تسريع',\n",
              " 'بلبنان',\n",
              " 'عامل',\n",
              " 'ترهيب',\n",
              " 'عتاگ',\n",
              " 'علو',\n",
              " 'لـشـوفـتـه',\n",
              " 'مئير',\n",
              " 'مراكش',\n",
              " 'موش',\n",
              " 'ظاهر',\n",
              " 'خرابيط',\n",
              " 'استرازينيكا',\n",
              " 'نبلش',\n",
              " 'تيبلش',\n",
              " 'فرمل',\n",
              " 'IYFV2021',\n",
              " 'سيطا',\n",
              " 'TeNTV',\n",
              " 'آبي',\n",
              " 'فنان',\n",
              " 'تصيب',\n",
              " 'دبلوماسيا',\n",
              " 'منتقد',\n",
              " 'قانو',\n",
              " 'موازن',\n",
              " 'اشتك',\n",
              " 'اجتهاد',\n",
              " 'شوشر',\n",
              " 'مسحوب',\n",
              " 'استخفاف',\n",
              " 'نايف',\n",
              " 'قلب',\n",
              " 'زحف',\n",
              " 'حيل',\n",
              " 'حكم',\n",
              " 'إحتجاج',\n",
              " 'هدأ',\n",
              " 'متغيران',\n",
              " '436',\n",
              " 'ريع',\n",
              " 'إغلاق',\n",
              " 'متمثلة',\n",
              " 'اد',\n",
              " 'باليورانيوم',\n",
              " 'understand',\n",
              " 'مقاس',\n",
              " 'ستقبل',\n",
              " 'رمز',\n",
              " 'نديد',\n",
              " 'تعض',\n",
              " 'دولار',\n",
              " 'سلوق',\n",
              " 'نتش',\n",
              " 'في9يناير2021',\n",
              " 'عمل',\n",
              " '6جرع',\n",
              " 'خيارا',\n",
              " 'يستغرق',\n",
              " 'خنزير',\n",
              " 'أرق',\n",
              " 'ونا',\n",
              " '٢٨',\n",
              " 'ياشمس',\n",
              " 'مهايط',\n",
              " 'ماخد',\n",
              " 'غصب',\n",
              " 'تلقائ',\n",
              " 'مازالتم',\n",
              " 'ماب',\n",
              " 'مخلوق',\n",
              " 'مخ',\n",
              " 'يالربع',\n",
              " 'اومن',\n",
              " 'يفرل',\n",
              " 'ريمـ',\n",
              " 'يرقراط',\n",
              " 'مخزون',\n",
              " 'ابراهيم',\n",
              " 'جيف',\n",
              " 'حجاب',\n",
              " 'نانوف',\n",
              " 'ميكونش',\n",
              " 'قلص',\n",
              " 'حربوق',\n",
              " 'اعطائ',\n",
              " 'التكنوقراط',\n",
              " 'یتم',\n",
              " 'ماك',\n",
              " 'حر',\n",
              " 'معجبه',\n",
              " '643',\n",
              " 'ضايح',\n",
              " 'نفس',\n",
              " 'تلقو',\n",
              " '275',\n",
              " 'يخلون',\n",
              " 'تقلب',\n",
              " 'ـNBA',\n",
              " 'تغط',\n",
              " 'HouthiTerorismInYemen',\n",
              " 'عسر',\n",
              " 'متابعه',\n",
              " 'سنة',\n",
              " 'رشاق',\n",
              " 'قار',\n",
              " 'فضل',\n",
              " 'مكابي',\n",
              " 'مخول',\n",
              " 'اعدام',\n",
              " 'هياخدو',\n",
              " 'إيمانويل',\n",
              " 'مؤرخة',\n",
              " 'طاهر',\n",
              " 'محصور',\n",
              " 'عيناء',\n",
              " 'نيئي',\n",
              " 'ماشى',\n",
              " 'يديو24',\n",
              " '٪🤔',\n",
              " 'قاد',\n",
              " 'استیراد',\n",
              " 'مؤيد',\n",
              " 'حديثا',\n",
              " 'سوا',\n",
              " 'تقطـت',\n",
              " 'مسومة',\n",
              " 'منتها',\n",
              " 'ضائ',\n",
              " 'حقول',\n",
              " 'خياشم',\n",
              " 'حكي',\n",
              " 'بتكنولوجيا',\n",
              " 'ياء',\n",
              " 'FEMA',\n",
              " 'صح',\n",
              " 'ضاه',\n",
              " 'انسان',\n",
              " 'جغراف',\n",
              " 'أطبائ',\n",
              " 'قلوب',\n",
              " 'غاف',\n",
              " 'شخلول',\n",
              " 'متمكن',\n",
              " 'توانس',\n",
              " 'بأ',\n",
              " 'أجواء',\n",
              " 'قررت',\n",
              " 'جوق',\n",
              " 'اكسفور',\n",
              " 'اقطاب',\n",
              " 'Johnson',\n",
              " 'ستاذ',\n",
              " 'دين',\n",
              " 'تعافى',\n",
              " 'مط',\n",
              " 'مارسيلي',\n",
              " 'حب',\n",
              " 'اذن',\n",
              " 'وريس',\n",
              " 'يصير',\n",
              " 'استرداد',\n",
              " 'زلام',\n",
              " 'اهل',\n",
              " 'خطار',\n",
              " 'شراب',\n",
              " 'ياس',\n",
              " 'مرسال',\n",
              " 'انقاذ',\n",
              " 'حفز',\n",
              " 'ملاعب',\n",
              " 'حاول',\n",
              " 'مطرح',\n",
              " 'تأكد',\n",
              " 'OrientPlus',\n",
              " 'متمن',\n",
              " 'يث',\n",
              " 'عنز',\n",
              " 'خفاش',\n",
              " 'حكامه',\n",
              " 'رايب',\n",
              " 'أعوان',\n",
              " 'نيوآرك',\n",
              " 'مقرب',\n",
              " 'via',\n",
              " 'يصـور',\n",
              " 'معتلق',\n",
              " 'هيكل',\n",
              " 'عتيب',\n",
              " 'استطع',\n",
              " 'أت',\n",
              " 'مفعم',\n",
              " 'مكتب',\n",
              " 'COVISHIELO',\n",
              " 'صيادل',\n",
              " 'عموم',\n",
              " 'شفي',\n",
              " 'VIP',\n",
              " 'منشآت',\n",
              " 'أثن',\n",
              " 'شجع',\n",
              " 'الحوامل',\n",
              " 'اهلاو',\n",
              " 'ينذكر',\n",
              " 'عاج',\n",
              " 'غضب',\n",
              " 'ـ18',\n",
              " 'منديرش',\n",
              " 'متوف',\n",
              " 'فارغ',\n",
              " 'رصد',\n",
              " 'اعوا',\n",
              " 'مسن',\n",
              " 'قليل',\n",
              " 'قارب',\n",
              " 'تدريس',\n",
              " 'اكيد',\n",
              " 'ورز',\n",
              " 'أطباء',\n",
              " 'روز',\n",
              " 'اعلن',\n",
              " 'هتفضل',\n",
              " 'اباد',\n",
              " 'وسع',\n",
              " 'سوينغ',\n",
              " 'حاش',\n",
              " 'آرائكن',\n",
              " 'متوطوع',\n",
              " 'شب',\n",
              " 'خصم',\n",
              " 'مستعملة',\n",
              " 'فع',\n",
              " 'منصور',\n",
              " 'مسرطن',\n",
              " 'مستنظر',\n",
              " 'اثن',\n",
              " 'علما',\n",
              " 'تضرعو',\n",
              " 'اعد',\n",
              " '03',\n",
              " 'أمصار',\n",
              " 'قاحا',\n",
              " 'سيمنسك',\n",
              " 'وربا',\n",
              " 'جريء',\n",
              " 'يستثن',\n",
              " '401',\n",
              " '٥٢',\n",
              " 'سريع',\n",
              " '🩸',\n",
              " 'سارس1',\n",
              " 'سني',\n",
              " 'حذ',\n",
              " 'MBC',\n",
              " '٢٦',\n",
              " 'منعم',\n",
              " 'هيئا',\n",
              " 'سايونار',\n",
              " 'معانق',\n",
              " 'دلائل',\n",
              " 'امير',\n",
              " 'مجزره',\n",
              " 'حيال',\n",
              " 'هناك',\n",
              " 'سفل',\n",
              " 'لذل',\n",
              " 'هالملي',\n",
              " 'ضعفا',\n",
              " 'ضعيفة',\n",
              " 'ومي',\n",
              " 'إيمان',\n",
              " 'مسموح',\n",
              " 'غذي',\n",
              " 'مخالط',\n",
              " 'info',\n",
              " '7',\n",
              " 'مسامح',\n",
              " 'تحصل',\n",
              " 'Institute',\n",
              " 'رص',\n",
              " 'اختف',\n",
              " 'آلم',\n",
              " 'مقطوع',\n",
              " 'شيع',\n",
              " 'إكتشاف',\n",
              " 'رمال',\n",
              " 'واليد',\n",
              " 'مبكر',\n",
              " 'زاول',\n",
              " 'خنوع',\n",
              " 'ستحو',\n",
              " 'سيب',\n",
              " 'قعدو',\n",
              " '253جرع',\n",
              " 'مختلف',\n",
              " 'اثار',\n",
              " 'ياعمر',\n",
              " 'فقه',\n",
              " 'منقذ',\n",
              " 'Zombies',\n",
              " 'مجلس',\n",
              " 'مقاطع',\n",
              " 'رئو',\n",
              " 'ساسه',\n",
              " 'تبريك',\n",
              " 'دعاؤ',\n",
              " 'تابع',\n",
              " 'fishing',\n",
              " 'China',\n",
              " 'هافت',\n",
              " 'شم',\n",
              " 'قوان',\n",
              " '12يور',\n",
              " 'قه',\n",
              " 'وه',\n",
              " 'اعراض',\n",
              " 'قناع',\n",
              " 'مح',\n",
              " 'شاب',\n",
              " 'طبيب',\n",
              " 'جح',\n",
              " '4G',\n",
              " 'يسعد',\n",
              " 'هيبع',\n",
              " 'بنوك',\n",
              " 'متفجر',\n",
              " 'لغي',\n",
              " 'صرام',\n",
              " '🥳',\n",
              " 'أعضاء',\n",
              " 'اعتناء',\n",
              " 'مناظير',\n",
              " ...]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku3lg9H8pWl-"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfl03iEpWl-"
      },
      "source": [
        "## 1. TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "p1iEoxZepWl_",
        "outputId": "890cefd1-4d2f-4349-f1f9-fa27aafea987"
      },
      "outputs": [],
      "source": [
        "ngramdata_features, word_vectorizer = get_ngram_features(train_data)\n",
        "ngramdata_features.head()\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'TFIDFVectorizer.sav'\n",
        "pickle.dump(word_vectorizer, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rJZAOHGpWl_"
      },
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUdWIXKjpWl_",
        "outputId": "ffb4a071-f537-4938-b39a-35fbfddac3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.48      0.14      0.22        70\n",
            "           0       0.45      0.28      0.34       126\n",
            "           1       0.85      0.96      0.90       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.59      0.46      0.49      1000\n",
            "weighted avg       0.78      0.81      0.78      1000\n",
            "\n",
            "accuracy: 0.814\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC()\n",
        "pipe_tfidf = make_pipeline(word_vectorizer, clf)\n",
        "pipe_tfidf.fit(train_data['text'], train_data['stance'])\n",
        "print_report(pipe_tfidf, test_data['text'], test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'LinearSVC_tfidf.sav'\n",
        "pickle.dump(pipe_tfidf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMiuOYKLpWmA"
      },
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxs8cWCrpWmA",
        "outputId": "ed301a76-ef87-4ebf-e862-f80e1fbeadba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.52      0.17      0.26        70\n",
            "           0       0.44      0.23      0.30       126\n",
            "           1       0.84      0.95      0.89       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.60      0.45      0.48      1000\n",
            "weighted avg       0.77      0.81      0.77      1000\n",
            "\n",
            "accuracy: 0.807\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "rf = RandomForestClassifier()\n",
        "rf_tfidf = rf.fit(X_train_tfidf, train_data['stance'])\n",
        "y_pred = rf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print_report(rf_tfidf, X_test_tfidf, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'RandomForest_tfidf.sav'\n",
        "pickle.dump(rf_tfidf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.30      0.44      0.36        70\n",
            "           0       0.30      0.63      0.41       126\n",
            "           1       0.94      0.74      0.83       804\n",
            "\n",
            "    accuracy                           0.71      1000\n",
            "   macro avg       0.52      0.61      0.53      1000\n",
            "weighted avg       0.82      0.71      0.74      1000\n",
            "\n",
            "accuracy: 0.707\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "CNB_clr = ComplementNB()\n",
        "CNB_clr_tfidf = CNB_clr.fit(X_train_tfidf, train_data['stance'])\n",
        "print_report(CNB_clr_tfidf, X_test_tfidf, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'NaiveBayes_tfidf.sav'\n",
        "pickle.dump(CNB_clr, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeKP3mYvpWmA"
      },
      "source": [
        "## 2.CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\feature_extraction.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "e:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\feature_extraction.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        }
      ],
      "source": [
        "X_train_vect_avg, X_test_vect_avg = get_word_embedding_features(train_data, test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.21      0.11      0.15        70\n",
            "           0       0.27      0.17      0.21       126\n",
            "           1       0.83      0.91      0.87       804\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.44      0.40      0.41      1000\n",
            "weighted avg       0.72      0.76      0.74      1000\n",
            "\n",
            "accuracy: 0.761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC(class_weight={-1:0.66, 0:0.29, 1:0.05})\n",
        "clf.fit(X_train_vect_avg, train_data['stance'])\n",
        "print_report(clf, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'LinearSVC_CBOW.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.40      0.06      0.10        70\n",
            "           0       0.48      0.08      0.14       126\n",
            "           1       0.82      0.99      0.89       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.56      0.37      0.38      1000\n",
            "weighted avg       0.75      0.81      0.74      1000\n",
            "\n",
            "accuracy: 0.806\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
        "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'RandomForest_CBOW.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.Ara2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQacWVpg01Q-",
        "outputId": "9b4558b1-951e-4de2-9fb6-ed28a2479908"
      },
      "outputs": [],
      "source": [
        "train_data_embeddings = np.array([np.array([nlp(i).vector for i in ls if i in vocab]) for ls in train_data[\"text\"]], dtype=object)\n",
        "test_data_embeddings = np.array([np.array([nlp(i).vector for i in ls if i in vocab]) for ls in test_data[\"text\"]], dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vLxZAs4QpWmA"
      },
      "outputs": [],
      "source": [
        "X_train_vect_avg, X_test_vect_avg = avg_word_vector(train_data_embeddings, test_data_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN3Nc-kKpWmB"
      },
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQzpnk2SpWmB",
        "outputId": "6ee5af9d-bf13-47eb-e4d2-45494519cb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.20      0.11      0.14        70\n",
            "           0       0.26      0.17      0.20       126\n",
            "           1       0.83      0.91      0.87       804\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.43      0.40      0.41      1000\n",
            "weighted avg       0.71      0.76      0.73      1000\n",
            "\n",
            "accuracy: 0.759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC(class_weight={-1:0.66, 0:0.29, 1:0.05})\n",
        "clf.fit(X_train_vect_avg, train_data['stance'])\n",
        "print_report(clf, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'LinearSVC_Ara2Vec.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egK5dQLFpWmB"
      },
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYAptW2opWmC",
        "outputId": "512b8b28-2267-42c3-b421-17134a1bb295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.40      0.06      0.10        70\n",
            "           0       0.41      0.07      0.12       126\n",
            "           1       0.82      0.99      0.89       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.54      0.37      0.37      1000\n",
            "weighted avg       0.74      0.81      0.74      1000\n",
            "\n",
            "accuracy: 0.805\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
        "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'RandomForest_Ara2Vec.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create initial embedding matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkAjiyv18q-n",
        "outputId": "d3d61811-74b2-4af2-bf60-c08023abf629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([12538, 100])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_train_matrix = []\n",
        "for word in vocab:\n",
        "  weights_train_matrix.append(nlp(word).vector)\n",
        "\n",
        "weights_train_matrix = torch.from_numpy(np.array(weights_train_matrix))\n",
        "weights_train_matrix.size()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu7R_QRYpWmK"
      },
      "source": [
        "# ArabicDataset\n",
        "The class that impelements the dataset for arabic tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZZ_dc9dpWmL",
        "outputId": "3197b7ab-8c17-493d-f5b1-82254f19b461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (embedding): Embedding(12538, 100)\n",
            "  (GRU): GRU(100, 100, batch_first=True)\n",
            "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Classifier(weights_train_matrix)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXO2KD-pWmM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ehn-xNeJpWmM"
      },
      "outputs": [],
      "source": [
        "train_data_tokenized_as_num = train_data_tokenized.apply(lambda x: [word2index[word] for word in x])\n",
        "# apply the same tokenization to the test set\n",
        "test_data_tokenized_as_num = test_data_tokenized.apply(lambda x: [word2index[word] for word in x if word in word2index])\n",
        "train_dataset = ArabicDataset(list(train_data_tokenized_as_num), train_data['stance'] + 1, word2index['<فراغ>'])\n",
        "test_dataset = ArabicDataset(list(test_data_tokenized_as_num), test_data['stance'] + 1, word2index['<فراغ>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IItLwKkpWmN",
        "outputId": "c2688cfc-9911-464b-b577-e824840c966a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:37<00:00,  5.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | Train Loss: 1.0335997639725742         | Train Accuracy: 0.756153404712677\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:46<00:00,  4.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2 | Train Loss: 1.0325850939641805         | Train Accuracy: 0.788637638092041\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:38<00:00,  5.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3 | Train Loss: 1.0277269283930461         | Train Accuracy: 0.7925014495849609\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:33<00:00,  6.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 4 | Train Loss: 1.015122158886635         | Train Accuracy: 0.7787635922431946\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:35<00:00,  6.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 5 | Train Loss: 0.8301106843774178         | Train Accuracy: 0.6993417143821716\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:31<00:00,  7.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 6 | Train Loss: 0.7372453431832736         | Train Accuracy: 0.7389811277389526\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:30<00:00,  7.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 7 | Train Loss: 0.6447076398760216         | Train Accuracy: 0.77017742395401\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:31<00:00,  7.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 8 | Train Loss: 0.5416968296107636         | Train Accuracy: 0.7995134592056274\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:32<00:00,  6.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 9 | Train Loss: 0.45136929960011346         | Train Accuracy: 0.8360045552253723\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:28<00:00,  7.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 10 | Train Loss: 0.3356938823785412         | Train Accuracy: 0.8674871325492859\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_dataset, epochs=10, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to disk\n",
        "filename = './models/GRU_Ara2Vec.pth'\n",
        "torch.save(model.state_dict(), filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNB-SaT6pWmN"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gru_model = Classifier(weights_train_matrix)\n",
        "gru_model.load_state_dict(torch.load('./models/GRU_Ara2Vec.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3431, 7390, 3439, 4258, 12005, 11069, 5453, 8316, 10197, 6234, 3518, 2929, 2890, 2246, 9764, 5109, 3431, 7390, 1235, 7095, 2574, 11453, 10321, 2246, 21, 4258, 12005, 3439, 5253, 5453, 8316, 11123, 7187, 3493, 2351, 5453, 2929, 3406]\n",
            "(tensor([ 3431,  7390,  3439,  4258, 12005, 11069,  5453,  8316, 10197,  6234,\n",
            "         3518,  2929,  2890,  2246,  9764,  5109,  3431,  7390,  1235,  7095,\n",
            "         2574, 11453, 10321,  2246,    21,  4258, 12005,  3439,  5253,  5453,\n",
            "         8316, 11123,  7187,  3493,  2351,  5453,  2929,  3406, 12537, 12537,\n",
            "        12537, 12537, 12537, 12537, 12537, 12537, 12537, 12537, 12537, 12537,\n",
            "        12537, 12537, 12537, 12537], dtype=torch.int32), tensor(2))\n"
          ]
        }
      ],
      "source": [
        "print(test_data_tokenized_as_num[0])\n",
        "print(test_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Z1SWhqpWmO",
        "outputId": "eb0cef84-dc9c-4b6c-a4f5-bfbb6872ad77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:01<00:00, 30.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.730\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.27      0.29        70\n",
            "           1       0.30      0.60      0.40       126\n",
            "           2       0.92      0.79      0.85       804\n",
            "\n",
            "    accuracy                           0.73      1000\n",
            "   macro avg       0.51      0.55      0.51      1000\n",
            "weighted avg       0.80      0.73      0.76      1000\n",
            "\n",
            "\n",
            "Test Accuracy: 0.7300000190734863\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-0.25917843,  1.371456  , -1.413901  ],\n",
              "       [-3.7941768 ,  0.1537084 ,  4.186266  ],\n",
              "       [-4.1633673 ,  0.1650067 ,  4.571656  ],\n",
              "       ...,\n",
              "       [-3.1534357 ,  0.28026307,  3.231864  ],\n",
              "       [-0.9125517 ,  0.8395538 , -0.2619845 ],\n",
              "       [-1.263289  ,  0.06857997,  1.1880075 ]], dtype=float32)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(gru_model, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "62cd17edec06c1bcb7cce561853235234094d242005d116fab77979ddb024dcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
