{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "294CcummpWl2",
        "outputId": "b6146a3d-18b7-4510-ab60-2bb5d8e82f2b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32me:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\model_building.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/model_building.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/model_building.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR8uhavw73yb",
        "outputId": "d483f37b-64a5-4dda-8d4a-5e50dd066b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nlp_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/nlp_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U139J06L2VSB",
        "outputId": "f678c9c5-9f66-471f-aca6-3abc474a0923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic-stopwords in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from arabic-stopwords) (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic>=0.6.2->arabic-stopwords) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: qalsadi in /usr/local/lib/python3.8/dist-packages (0.4.5)\n",
            "Requirement already satisfied: naftawayh>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: Arabic-Stopwords>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: alyahmor>=0.1 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.1.5)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.16.0)\n",
            "Requirement already satisfied: pickledb>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.9.2)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.6.15)\n",
            "Requirement already satisfied: arramooz-pysqlite>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: tashaphyne>=0.3.4.1 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.15.0)\n",
            "Requirement already satisfied: libqutrub>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install arabic-stopwords\n",
        "!pip install qalsadi\n",
        "!pip install pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giRMTMSppgrI",
        "outputId": "1e5469a5-198b-49d3-ee15-ba7a3d0fde3e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string\n",
        "import qalsadi.lemmatizer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from pyarabic.araby import tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "from pre_processing_post import processPost\n",
        "from feature_extraction import get_ngram_features, get_word_embedding_features, avg_word_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8V_Ralq6pWl4"
      },
      "outputs": [],
      "source": [
        "# needed functions\n",
        "def print_report(pipe, x_test, y_test):\n",
        "    y_pred = pipe.predict(x_test)\n",
        "    report = metrics.classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EsnK9zupWl5"
      },
      "source": [
        "# Read train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "dlD_8DNapWl6",
        "outputId": "5c801c19-c001-4554-88ba-8c560c4914ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('./DataSet/train.csv',sep=',',header=0)\n",
        "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "a-m343EOpWl8",
        "outputId": "a8d192b6-009a-499f-8545-bc1903d0ca9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...         1       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...         2       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...         2       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...         1       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...         4       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...         2       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...         2       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...         4       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...         9       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...         2       1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#remove first row that has the header\n",
        "train_data['category'] = train_data['category'].astype('category').cat.codes\n",
        "train_data.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Po_uSrO-T6sS"
      },
      "source": [
        "## Over Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jf6OGm-JNNAm"
      },
      "outputs": [],
      "source": [
        "# !pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AMwOtKGOpWl8"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "# train_data=train_data.drop('category',axis=1)\n",
        "# y=train_data['stance']\n",
        "# print(Counter(train_data['stance']))\n",
        "# train_data=train_data.drop('stance',axis=1)\n",
        "# # define oversampling strategy\n",
        "# oversample = RandomOverSampler(random_state=3)\n",
        "# # fit and apply the transform\n",
        "# train_data[\"text\"], train_data['stance'] = oversample.fit_resample(train_data, y)\n",
        "# print(Counter(train_data['stance']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3SV1t53pWl9"
      },
      "source": [
        "# Pre-Processing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWbuBoDgpWl9",
        "outputId": "dfe81e23-b7d5-4c71-a7a7-6202ef2aa394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية لقاح لعد ما اتابع الاخبار هم بكل مجالاتهم متفوقين وراح يطلع اللقاح قريباً؟<LF>#دعبول_دومه_مسحول\n",
            "دعبول حضر من نت طلب قائد دول إسلام قاح عد تابع اخبار مجال متفوق طلع قاح قريبا دعبول دوم مسحول\n"
          ]
        }
      ],
      "source": [
        "print(train_data[\"text\"][9])\n",
        "train_data[\"text\"] = train_data['text'].apply(lambda x: processPost(x))\n",
        "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))\n",
        "print(train_data[\"text\"][9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f26urbLgkiKi"
      },
      "source": [
        "## Ara2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EfJCUEzEkpA-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'ar_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# load AraVec Spacy model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PqSckdSVkvrh"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing Class\n",
        "class Preprocessor:\n",
        "    def __init__(self, tokenizer, **cfg):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, text):\n",
        "        # preprocessed = processPost(text)\n",
        "        return self.tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cUqO4-OFk3Ch"
      },
      "outputs": [],
      "source": [
        "# Apply the `Preprocessor` Class\n",
        "nlp.tokenizer = Preprocessor(nlp.tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c7WB_vA_sIO"
      },
      "source": [
        "## create vocablary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RcoWDCqQ_w5n"
      },
      "outputs": [],
      "source": [
        "train_data_tokenized = train_data['text'].apply(tokenize)\n",
        "test_data_tokenized = test_data['text'].apply(tokenize)\n",
        "#merge all the sentences in one list\n",
        "vocab = [item for sublist in train_data_tokenized for item in sublist]\n",
        "vocab = list(set(vocab))\n",
        "vocab.append('<فراغ>')\n",
        "vocab.insert(0, '<مجهول>')\n",
        "word2index = {word: i for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku3lg9H8pWl-"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfl03iEpWl-"
      },
      "source": [
        "## 1. TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "p1iEoxZepWl_",
        "outputId": "890cefd1-4d2f-4349-f1f9-fa27aafea987"
      },
      "outputs": [],
      "source": [
        "ngramdata_features, word_vectorizer = get_ngram_features(train_data)\n",
        "ngramdata_features.head()\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'TFIDFVectorizer.sav'\n",
        "pickle.dump(word_vectorizer, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rJZAOHGpWl_"
      },
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUdWIXKjpWl_",
        "outputId": "ffb4a071-f537-4938-b39a-35fbfddac3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.48      0.14      0.22        70\n",
            "           0       0.45      0.28      0.34       126\n",
            "           1       0.85      0.96      0.90       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.59      0.46      0.49      1000\n",
            "weighted avg       0.78      0.81      0.78      1000\n",
            "\n",
            "accuracy: 0.814\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC()\n",
        "pipe_tfidf = make_pipeline(word_vectorizer, clf)\n",
        "pipe_tfidf.fit(train_data['text'], train_data['stance'])\n",
        "print_report(pipe_tfidf, test_data['text'], test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'LinearSVC_tfidf.sav'\n",
        "pickle.dump(pipe_tfidf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMiuOYKLpWmA"
      },
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxs8cWCrpWmA",
        "outputId": "ed301a76-ef87-4ebf-e862-f80e1fbeadba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.52      0.17      0.26        70\n",
            "           0       0.44      0.23      0.30       126\n",
            "           1       0.84      0.95      0.89       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.60      0.45      0.48      1000\n",
            "weighted avg       0.77      0.81      0.77      1000\n",
            "\n",
            "accuracy: 0.807\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "rf = RandomForestClassifier()\n",
        "rf_tfidf = rf.fit(X_train_tfidf, train_data['stance'])\n",
        "y_pred = rf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print_report(rf_tfidf, X_test_tfidf, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'RandomForest_tfidf.sav'\n",
        "pickle.dump(rf_tfidf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.30      0.44      0.36        70\n",
            "           0       0.30      0.63      0.41       126\n",
            "           1       0.94      0.74      0.83       804\n",
            "\n",
            "    accuracy                           0.71      1000\n",
            "   macro avg       0.52      0.61      0.53      1000\n",
            "weighted avg       0.82      0.71      0.74      1000\n",
            "\n",
            "accuracy: 0.707\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = word_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "CNB_clr = ComplementNB()\n",
        "CNB_clr_tfidf = CNB_clr.fit(X_train_tfidf, train_data['stance'])\n",
        "print_report(CNB_clr_tfidf, X_test_tfidf, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'NaiveBayes_tfidf.sav'\n",
        "pickle.dump(CNB_clr, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeKP3mYvpWmA"
      },
      "source": [
        "## 2.CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\feature_extraction.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "e:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\feature_extraction.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        }
      ],
      "source": [
        "X_train_vect_avg, X_test_vect_avg = get_word_embedding_features(train_data, test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.21      0.11      0.15        70\n",
            "           0       0.27      0.17      0.21       126\n",
            "           1       0.83      0.91      0.87       804\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.44      0.40      0.41      1000\n",
            "weighted avg       0.72      0.76      0.74      1000\n",
            "\n",
            "accuracy: 0.761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC(class_weight={-1:0.66, 0:0.29, 1:0.05})\n",
        "clf.fit(X_train_vect_avg, train_data['stance'])\n",
        "print_report(clf, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'LinearSVC_CBOW.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.40      0.06      0.10        70\n",
            "           0       0.48      0.08      0.14       126\n",
            "           1       0.82      0.99      0.89       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.56      0.37      0.38      1000\n",
            "weighted avg       0.75      0.81      0.74      1000\n",
            "\n",
            "accuracy: 0.806\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
        "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'RandomForest_CBOW.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.Ara2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQacWVpg01Q-",
        "outputId": "9b4558b1-951e-4de2-9fb6-ed28a2479908"
      },
      "outputs": [],
      "source": [
        "train_data_embeddings = np.array([np.array([nlp(i).vector for i in ls if i in vocab]) for ls in train_data[\"text\"]], dtype=object)\n",
        "test_data_embeddings = np.array([np.array([nlp(i).vector for i in ls if i in vocab]) for ls in test_data[\"text\"]], dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vLxZAs4QpWmA"
      },
      "outputs": [],
      "source": [
        "X_train_vect_avg, X_test_vect_avg = avg_word_vector(train_data_embeddings, test_data_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN3Nc-kKpWmB"
      },
      "source": [
        "LinearSVC Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQzpnk2SpWmB",
        "outputId": "6ee5af9d-bf13-47eb-e4d2-45494519cb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.20      0.11      0.14        70\n",
            "           0       0.26      0.17      0.20       126\n",
            "           1       0.83      0.91      0.87       804\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.43      0.40      0.41      1000\n",
            "weighted avg       0.71      0.76      0.73      1000\n",
            "\n",
            "accuracy: 0.759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clf = LinearSVC(class_weight={-1:0.66, 0:0.29, 1:0.05})\n",
        "clf.fit(X_train_vect_avg, train_data['stance'])\n",
        "print_report(clf, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'LinearSVC_Ara2Vec.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egK5dQLFpWmB"
      },
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYAptW2opWmC",
        "outputId": "512b8b28-2267-42c3-b421-17134a1bb295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.40      0.06      0.10        70\n",
            "           0       0.41      0.07      0.12       126\n",
            "           1       0.82      0.99      0.89       804\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.54      0.37      0.37      1000\n",
            "weighted avg       0.74      0.81      0.74      1000\n",
            "\n",
            "accuracy: 0.805\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "rf_vect = rf.fit(X_train_vect_avg, train_data['stance'].values.ravel())\n",
        "print_report(rf_vect, X_test_vect_avg, test_data['stance'])\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'RandomForest_Ara2Vec.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create initial embedding matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkAjiyv18q-n",
        "outputId": "d3d61811-74b2-4af2-bf60-c08023abf629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([12538, 100])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_train_matrix = []\n",
        "for word in vocab:\n",
        "  weights_train_matrix.append(nlp(word).vector)\n",
        "\n",
        "weights_train_matrix = torch.from_numpy(np.array(weights_train_matrix))\n",
        "weights_train_matrix.size()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu7R_QRYpWmK"
      },
      "source": [
        "# ArabicDataset\n",
        "The class that impelements the dataset for arabic tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "woMgdAn4pWmK"
      },
      "outputs": [],
      "source": [
        "class ArabicDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x, y, pad):\n",
        "    \"\"\"\n",
        "    This is the constructor of the ArabicDataset\n",
        "    Inputs:\n",
        "    - x: a list of lists where each list contains the ids of the tokens\n",
        "    - y: a list of lists where each list contains the label of each token in the sentence\n",
        "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
        "    \"\"\"\n",
        "    list_len = [len(i) for i in x]\n",
        "    MAX_LENGTH = max(list_len) \n",
        "    for i in range(len(x)):\n",
        "      x[i] = np.pad(x[i], (0, MAX_LENGTH-len(x[i])), 'constant', constant_values=(pad))\n",
        "\n",
        "    self.x = torch.from_numpy(np.array(x)) \n",
        "    self.y = torch.from_numpy(np.array(y))\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    This function should return the length of the dataset (the number of sentences)\n",
        "    \"\"\"\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    This function returns a subset of the whole dataset\n",
        "    \"\"\"\n",
        "    return (self.x[idx], self.y[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rpW2mlwMpWmL"
      },
      "outputs": [],
      "source": [
        "def create_emb_layer(weights_train_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_train_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_train_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, vocab_size=len(vocab), embedding_dim=100, hidden_size=100, n_classes=3, n_layer=1):\n",
        "    \"\"\"\n",
        "    The constructor of our NER model\n",
        "    Inputs:\n",
        "    - vacab_size: the number of unique words\n",
        "    - embedding_dim: the embedding dimension\n",
        "    - n_classes: the number of final classes (tags)\n",
        "    \"\"\"\n",
        "    self.hidden_size = hidden_size\n",
        "    super(Classifier, self).__init__()\n",
        "    \n",
        "    self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_train_matrix, True)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.GRU = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True, num_layers=n_layer)\n",
        "\n",
        "    self.linear = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, sentences):\n",
        "    \"\"\"\n",
        "    This function does the forward pass of our model\n",
        "    Inputs:\n",
        "    - sentences: tensor of shape (batch_size, max_length)\n",
        "\n",
        "    Returns:\n",
        "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
        "    \"\"\"\n",
        "\n",
        "    final_output = None\n",
        "    final_output, _ = self.GRU(self.embedding(sentences))\n",
        "    final_output = final_output[:, -1, :]\n",
        "    final_output = self.linear(final_output)\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZZ_dc9dpWmL",
        "outputId": "3197b7ab-8c17-493d-f5b1-82254f19b461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (embedding): Embedding(12538, 100)\n",
            "  (GRU): GRU(100, 100, batch_first=True)\n",
            "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Classifier()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXO2KD-pWmM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NI3ZuwkNpWmM"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataset, batch_size=32, epochs=10, learning_rate=0.001):\n",
        "  \"\"\"\n",
        "  This function implements the training logic\n",
        "  Inputs:\n",
        "  - model: the model ot be trained\n",
        "  - train_dataset: the training set of type NERDataset\n",
        "  - batch_size: integer represents the number of examples per step\n",
        "  - epochs: integer represents the total number of epochs (full training pass)\n",
        "  - learning_rate: the learning rate to be used by the optimizer\n",
        "  \"\"\"\n",
        "\n",
        "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # (2) make the criterion cross entropy loss\n",
        "  criterion = nn.CrossEntropyLoss(weight=torch.tensor([.5, .4, .1])) \n",
        "\n",
        "  # (3) create the optimizer (Adam)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # GPU configuration\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "  for epoch_num in range(epochs):\n",
        "    if epoch_num % 5 == 0:\n",
        "      learning_rate /= 10\n",
        "    total_acc_train = 0\n",
        "    total_loss_train = 0\n",
        "\n",
        "    for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "      # (4) move the train input to the device\n",
        "      train_label = train_label.to(device)\n",
        "\n",
        "      # (5) move the train label to the device\n",
        "      train_input = train_input.to(device)\n",
        "\n",
        "      # (6) do the forward pass\n",
        "      output = model.forward(sentences=train_input)\n",
        "      \n",
        "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
        "      batch_loss = criterion(output, train_label) \n",
        "\n",
        "      # (8) append the batch loss to the total_loss_train\n",
        "      total_loss_train += batch_loss.item()\n",
        "      \n",
        "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
        "      argmax = torch.argmax(output, dim=1)\n",
        "      acc = torch.sum(torch.eq(argmax, train_label))\n",
        "      total_acc_train += acc\n",
        "\n",
        "      # (10) zero your gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # (11) do the backward pass\n",
        "      batch_loss.backward()\n",
        "\n",
        "      # (12) update the weights with your optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "    num_of_batches = len(train_dataset) / batch_size\n",
        "    num_of_batches = int(num_of_batches) + 1\n",
        "    # epoch loss\n",
        "    epoch_loss = total_loss_train / num_of_batches\n",
        "    \n",
        "    # (13) calculate the accuracy\n",
        "    epoch_acc = total_acc_train / len(train_dataset)\n",
        "\n",
        "    print(\n",
        "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
        "        | Train Accuracy: {epoch_acc}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ehn-xNeJpWmM"
      },
      "outputs": [],
      "source": [
        "train_data_tokenized_as_num = train_data_tokenized.apply(lambda x: [word2index[word] for word in x])\n",
        "# apply the same tokenization to the test set\n",
        "test_data_tokenized_as_num = test_data_tokenized.apply(lambda x: [word2index[word] for word in x if word in word2index])\n",
        "train_dataset = ArabicDataset(list(train_data_tokenized_as_num), train_data['stance'] + 1, word2index['<فراغ>'])\n",
        "test_dataset = ArabicDataset(list(test_data_tokenized_as_num), test_data['stance'] + 1, word2index['<فراغ>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IItLwKkpWmN",
        "outputId": "c2688cfc-9911-464b-b577-e824840c966a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:18<00:00, 12.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | Train Loss: 1.0329111667528545         | Train Accuracy: 0.7449914216995239\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2 | Train Loss: 1.0293194286899479         | Train Accuracy: 0.7925014495849609\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3 | Train Loss: 0.88802936659556         | Train Accuracy: 0.6971951723098755\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 4 | Train Loss: 0.7924346740115179         | Train Accuracy: 0.7173726558685303\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:18<00:00, 11.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 5 | Train Loss: 0.722890184214126         | Train Accuracy: 0.7445620894432068\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 6 | Train Loss: 0.6591148775870397         | Train Accuracy: 0.7638809680938721\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:18<00:00, 11.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 7 | Train Loss: 0.5924284098899528         | Train Accuracy: 0.7831997871398926\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 8 | Train Loss: 0.5115706066698789         | Train Accuracy: 0.8148254156112671\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 9 | Train Loss: 0.42234649260838825         | Train Accuracy: 0.8457355499267578\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 219/219 [00:19<00:00, 11.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 10 | Train Loss: 0.3685627749793606         | Train Accuracy: 0.8567544221878052\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_dataset, epochs=10, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to disk\n",
        "filename = './models/GRU_Ara2Vec.sav'\n",
        "torch.save(model.state_dict(), filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNB-SaT6pWmN"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nt3ke5I4pWmN"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataset, batch_size=32):\n",
        "  \"\"\"\n",
        "  This function takes a model and evaluates its performance (accuracy) on a test data\n",
        "  Inputs:\n",
        "  - model: the model\n",
        "  - test_dataset: dataset of type ArabicDataset\n",
        "  \"\"\"\n",
        "  \n",
        "  # (1) create the test data loader\n",
        "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # GPU Configuration\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "  total_acc_test = 0.0\n",
        "  \n",
        "  y_test = [] \n",
        "  y_predected = [] \n",
        "  # (2) disable gradients\n",
        "  with torch.no_grad():\n",
        "    report = None\n",
        "    for test_input, test_label in tqdm(test_dataloader):\n",
        "      # (3) move the test input to the device\n",
        "      test_label = test_label.to(device)\n",
        "\n",
        "      # (4) move the test label to the device\n",
        "      test_input = test_input.to(device)\n",
        "\n",
        "      # (5) do the forward pass\n",
        "      output = model.forward(sentences=test_input)\n",
        "\n",
        "      # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
        "      acc = torch.sum(torch.eq(torch.argmax(output, dim=1), test_label))\n",
        "      total_acc_test += acc\n",
        "      \n",
        "      # f1 score calculation\n",
        "      y_test +=(list(test_label.view(-1)))\n",
        "      y_predected +=(list(torch.argmax(output, dim=1).view(-1)))\n",
        "\n",
        "    # (6) calculate the over all accuracy\n",
        "    total_acc_test /= len(test_dataset)\n",
        "\n",
        "  report = metrics.classification_report(y_test, y_predected)\n",
        "  print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_predected)))\n",
        "  print(report)\n",
        "  \n",
        "  print(f'\\nTest Accuracy: {total_acc_test}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Z1SWhqpWmO",
        "outputId": "eb0cef84-dc9c-4b6c-a4f5-bfbb6872ad77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 43.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.721\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.34      0.29        70\n",
            "           1       0.29      0.51      0.37       126\n",
            "           2       0.93      0.79      0.85       804\n",
            "\n",
            "    accuracy                           0.72      1000\n",
            "   macro avg       0.49      0.55      0.50      1000\n",
            "weighted avg       0.80      0.72      0.75      1000\n",
            "\n",
            "\n",
            "Test Accuracy: 0.7210000157356262\n"
          ]
        }
      ],
      "source": [
        "evaluate(model, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "62cd17edec06c1bcb7cce561853235234094d242005d116fab77979ddb024dcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
