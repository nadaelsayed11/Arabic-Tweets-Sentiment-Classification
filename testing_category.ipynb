{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDXNRr4wmDHR",
        "outputId": "e687c6c7-d7ae-4a6c-9746-e832553cc754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPOfzKgXmFE1",
        "outputId": "5f460f7f-4470-4611-9e53-0583fe6c74bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nlp_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/nlp_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AD_9EnImXqM",
        "outputId": "40ff80fa-f91e-4ec4-cdc2-657506099d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.8/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic-stopwords in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from arabic-stopwords) (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic>=0.6.2->arabic-stopwords) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting qalsadi\n",
            "  Downloading qalsadi-0.4.5-py3-none-any.whl (256 kB)\n",
            "\u001b[K     |████████████████████████████████| 256 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting arramooz-pysqlite>=0.3\n",
            "  Downloading arramooz_pysqlite-0.3-py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting libqutrub>=1.2.3\n",
            "  Downloading libqutrub-1.2.4.1-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 80.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Arabic-Stopwords>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.15.0)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.6.15)\n",
            "Collecting pickledb>=0.9.2\n",
            "  Downloading pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "Collecting tashaphyne>=0.3.4.1\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.16.0)\n",
            "Collecting alyahmor>=0.1\n",
            "  Downloading alyahmor-0.1.5-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 557 kB/s \n",
            "\u001b[?25hCollecting naftawayh>=0.3\n",
            "  Downloading Naftawayh-0.4-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 62.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickledb\n",
            "  Building wheel for pickledb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickledb: filename=pickleDB-0.9.2-py3-none-any.whl size=4269 sha256=268b3395f8dc1eb927a7f1fc2cc4a824764dbf787ffabb4956000d275314ac33\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/91/d4/ef2e6a46ad2bc41f9cfad35fa2db5b34357a5e4da67c385ffa\n",
            "Successfully built pickledb\n",
            "Installing collected packages: tashaphyne, libqutrub, arramooz-pysqlite, pickledb, naftawayh, alyahmor, qalsadi\n",
            "Successfully installed alyahmor-0.1.5 arramooz-pysqlite-0.3 libqutrub-1.2.4.1 naftawayh-0.4 pickledb-0.9.2 qalsadi-0.4.5 tashaphyne-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarabic\n",
        "!pip install arabic-stopwords\n",
        "!pip install qalsadi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFSuYIRqmB7d"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from pyarabic.araby import tokenize\n",
        "import numpy as np\n",
        "import pickle\n",
        "import spacy\n",
        "import torch\n",
        "\n",
        "# from model_building import Classifier \n",
        "from pre_processing_post import processPost\n",
        "from feature_extraction import get_ngram_features, get_word_embedding_features, avg_word_vector\n",
        "from gru_model import ArabicDataset, Classifier, evaluate, train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyqggDmAmB7n"
      },
      "outputs": [],
      "source": [
        "# needed functions\n",
        "def print_report(y_pred, y_test):\n",
        "    report = metrics.classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PrWnSaoOmB7p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'ar_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# load AraVec Spacy model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eUBzKwr6mB7s"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('./DataSet/test.csv',sep=',',header=0)\n",
        "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))\n",
        "test_data['category'] = test_data['category'].astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "031sNg5cmB7t"
      },
      "outputs": [],
      "source": [
        "# load word2index dictionary\n",
        "with open('./vocab/category/word2index.pickle', 'rb') as f:\n",
        "    word2index = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvGi85mSmB7v",
        "outputId": "0396ec16-0565-4e71-f45d-6f82e84bc31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([12538, 100])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_train_matrix = []\n",
        "for word in word2index:\n",
        "  weights_train_matrix.append(nlp(word).vector)\n",
        "\n",
        "weights_train_matrix = torch.from_numpy(np.array(weights_train_matrix))\n",
        "weights_train_matrix.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dZeBxsmB7x"
      },
      "source": [
        "td-idf feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0ah4rU5XmB70"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LinearSVC' object has no attribute 'predict_proba'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32me:\\CMP\\seventh term\\Natural Language Processing\\Arabic-Tweets-Sentiment-Classification\\testing_category.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/testing_category.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     word_vectorizer \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/testing_category.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_test_tfidf \u001b[39m=\u001b[39m word_vectorizer\u001b[39m.\u001b[39mtransform(test_data[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CMP/seventh%20term/Natural%20Language%20Processing/Arabic-Tweets-Sentiment-Classification/testing_category.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_pred \u001b[39m=\u001b[39m SVC_model\u001b[39m.\u001b[39;49mpredict_proba(test_data[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:109\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[1;34m(self, obj, owner)\u001b[0m\n\u001b[0;32m    103\u001b[0m attr_err \u001b[39m=\u001b[39m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(owner\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute_name)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[39m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[39m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck(obj):\n\u001b[0;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[0;32m    112\u001b[0m     \u001b[39m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:45\u001b[0m, in \u001b[0;36m_final_estimator_has.<locals>.check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     44\u001b[0m     \u001b[39m# raise original `AttributeError` if `attr` does not exist\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator, attr)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'LinearSVC' object has no attribute 'predict_proba'"
          ]
        }
      ],
      "source": [
        "# load SVC model\n",
        "with open('./models/category/LinearSVC_tfidf.sav', 'rb') as f:\n",
        "    SVC_model = pickle.load(f)\n",
        "\n",
        "with open('./models/category/TFIDFVectorizer.sav', 'rb') as f:\n",
        "    word_vectorizer = pickle.load(f)\n",
        "\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "y_pred = SVC_model.predict_proba(test_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Wshd6OR0mB72"
      },
      "outputs": [],
      "source": [
        "with open('./models/category/RandomForest_tfidf.sav', 'rb') as f:\n",
        "    rf = pickle.load(f)\n",
        "\n",
        "y_pred += rf.predict_proba(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaUb1JKumB73",
        "outputId": "676da9af-d8a4-4fa6-84bc-c6aed62605d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 56.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.40      0.33        10\n",
            "           1       0.75      0.83      0.78       145\n",
            "           2       0.76      0.41      0.53       545\n",
            "           3       0.12      0.18      0.14        17\n",
            "           4       0.42      0.60      0.50       128\n",
            "           5       0.20      0.50      0.29        82\n",
            "           6       0.14      0.30      0.19        20\n",
            "           7       0.33      0.50      0.40         2\n",
            "           8       0.06      0.07      0.06        15\n",
            "           9       0.30      0.47      0.37        36\n",
            "\n",
            "    accuracy                           0.49      1000\n",
            "   macro avg       0.34      0.43      0.36      1000\n",
            "weighted avg       0.61      0.49      0.51      1000\n",
            "\n",
            "\n",
            "Test Accuracy: 0.492000013589859\n"
          ]
        }
      ],
      "source": [
        "#load GRU model\n",
        "with open('./models/category/GRU_Ara2Vec.pth', 'rb') as f:\n",
        "    gru_model = Classifier(weights_train_matrix, n_classes=10, n_layer=2)\n",
        "    gru_model.load_state_dict(torch.load(f)) \n",
        "    gru_model.eval()\n",
        "\n",
        "test_data_tokenized = test_data['text'].apply(tokenize)\n",
        "test_data_tokenized_as_num = test_data_tokenized.apply(lambda x: [word2index[word] for word in x if word in word2index])\n",
        "test_dataset = ArabicDataset(list(test_data_tokenized_as_num), test_data['category'], word2index['<فراغ>'])\n",
        "y_pred += evaluate(gru_model,test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qtAycDRmB75",
        "outputId": "23f8a08f-bb5e-4025-d63f-0c6581ef9beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.30      0.29        10\n",
            "           1       0.80      0.84      0.82       145\n",
            "           2       0.74      0.73      0.74       545\n",
            "           3       0.08      0.06      0.07        17\n",
            "           4       0.47      0.58      0.52       128\n",
            "           5       0.23      0.07      0.11        82\n",
            "           6       0.13      0.20      0.16        20\n",
            "           7       0.50      0.50      0.50         2\n",
            "           8       0.08      0.07      0.07        15\n",
            "           9       0.30      0.47      0.37        36\n",
            "\n",
            "    accuracy                           0.63      1000\n",
            "   macro avg       0.36      0.38      0.36      1000\n",
            "weighted avg       0.62      0.63      0.62      1000\n",
            "\n",
            "accuracy: 0.628\n"
          ]
        }
      ],
      "source": [
        "y_pred /= 3\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print_report(y_pred, test_data['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pv2h7xIsqSL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "62cd17edec06c1bcb7cce561853235234094d242005d116fab77979ddb024dcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
