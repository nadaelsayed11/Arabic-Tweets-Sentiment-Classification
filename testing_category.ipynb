{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDXNRr4wmDHR",
        "outputId": "e687c6c7-d7ae-4a6c-9746-e832553cc754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPOfzKgXmFE1",
        "outputId": "5f460f7f-4470-4611-9e53-0583fe6c74bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nlp_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/nlp_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AD_9EnImXqM",
        "outputId": "40ff80fa-f91e-4ec4-cdc2-657506099d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.8/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic-stopwords in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from arabic-stopwords) (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic>=0.6.2->arabic-stopwords) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting qalsadi\n",
            "  Downloading qalsadi-0.4.5-py3-none-any.whl (256 kB)\n",
            "\u001b[K     |████████████████████████████████| 256 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting arramooz-pysqlite>=0.3\n",
            "  Downloading arramooz_pysqlite-0.3-py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting libqutrub>=1.2.3\n",
            "  Downloading libqutrub-1.2.4.1-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 80.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Arabic-Stopwords>=0.3 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (1.15.0)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.6.15)\n",
            "Collecting pickledb>=0.9.2\n",
            "  Downloading pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "Collecting tashaphyne>=0.3.4.1\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from qalsadi) (0.16.0)\n",
            "Collecting alyahmor>=0.1\n",
            "  Downloading alyahmor-0.1.5-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 557 kB/s \n",
            "\u001b[?25hCollecting naftawayh>=0.3\n",
            "  Downloading Naftawayh-0.4-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 62.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickledb\n",
            "  Building wheel for pickledb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickledb: filename=pickleDB-0.9.2-py3-none-any.whl size=4269 sha256=268b3395f8dc1eb927a7f1fc2cc4a824764dbf787ffabb4956000d275314ac33\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/91/d4/ef2e6a46ad2bc41f9cfad35fa2db5b34357a5e4da67c385ffa\n",
            "Successfully built pickledb\n",
            "Installing collected packages: tashaphyne, libqutrub, arramooz-pysqlite, pickledb, naftawayh, alyahmor, qalsadi\n",
            "Successfully installed alyahmor-0.1.5 arramooz-pysqlite-0.3 libqutrub-1.2.4.1 naftawayh-0.4 pickledb-0.9.2 qalsadi-0.4.5 tashaphyne-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarabic\n",
        "!pip install arabic-stopwords\n",
        "!pip install qalsadi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFSuYIRqmB7d"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from pyarabic.araby import tokenize\n",
        "import numpy as np\n",
        "import pickle\n",
        "import spacy\n",
        "import torch\n",
        "\n",
        "# from model_building import Classifier \n",
        "from pre_processing_post import processPost\n",
        "from feature_extraction import get_ngram_features, get_word_embedding_features, avg_word_vector\n",
        "from gru_model import ArabicDataset, Classifier, evaluate, train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyqggDmAmB7n"
      },
      "outputs": [],
      "source": [
        "# needed functions\n",
        "def print_report(y_pred, y_test):\n",
        "    report = metrics.classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PrWnSaoOmB7p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'ar_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# load AraVec Spacy model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eUBzKwr6mB7s"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
        "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))\n",
        "test_data['category'] = test_data['category'].astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "031sNg5cmB7t"
      },
      "outputs": [],
      "source": [
        "# load word2index dictionary\n",
        "with open('./vocab/category/word2index.pickle', 'rb') as f:\n",
        "    word2index = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvGi85mSmB7v",
        "outputId": "0396ec16-0565-4e71-f45d-6f82e84bc31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([12538, 100])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_train_matrix = []\n",
        "for word in word2index:\n",
        "  weights_train_matrix.append(nlp(word).vector)\n",
        "\n",
        "weights_train_matrix = torch.from_numpy(np.array(weights_train_matrix))\n",
        "weights_train_matrix.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dZeBxsmB7x"
      },
      "source": [
        "td-idf feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0ah4rU5XmB70"
      },
      "outputs": [],
      "source": [
        "# load SVC model\n",
        "with open('./models/category/LinearSVC_tfidf.sav', 'rb') as f:\n",
        "    SVC_model = pickle.load(f)\n",
        "\n",
        "with open('./models/category/TFIDFVectorizer.sav', 'rb') as f:\n",
        "    word_vectorizer = pickle.load(f)\n",
        "\n",
        "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
        "y_pred = SVC_model.predict_proba(test_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Wshd6OR0mB72"
      },
      "outputs": [],
      "source": [
        "with open('./models/category/RandomForest_tfidf.sav', 'rb') as f:\n",
        "    rf = pickle.load(f)\n",
        "\n",
        "y_pred_ = rf.predict_proba(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaUb1JKumB73",
        "outputId": "676da9af-d8a4-4fa6-84bc-c6aed62605d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.40      0.33        10\n",
            "           1       0.75      0.83      0.78       145\n",
            "           2       0.76      0.41      0.53       545\n",
            "           3       0.12      0.18      0.14        17\n",
            "           4       0.42      0.60      0.50       128\n",
            "           5       0.20      0.50      0.29        82\n",
            "           6       0.14      0.30      0.19        20\n",
            "           7       0.33      0.50      0.40         2\n",
            "           8       0.06      0.07      0.06        15\n",
            "           9       0.30      0.47      0.37        36\n",
            "\n",
            "    accuracy                           0.49      1000\n",
            "   macro avg       0.34      0.43      0.36      1000\n",
            "weighted avg       0.61      0.49      0.51      1000\n",
            "\n",
            "\n",
            "Test Accuracy: 0.492000013589859\n"
          ]
        }
      ],
      "source": [
        "#load GRU model\n",
        "with open('./models/category/GRU_Ara2Vec.pth', 'rb') as f:\n",
        "    gru_model = Classifier(weights_train_matrix, n_classes=10, n_layer=2)\n",
        "    gru_model.load_state_dict(torch.load(f)) \n",
        "    gru_model.eval()\n",
        "\n",
        "test_data_tokenized = test_data['text'].apply(tokenize)\n",
        "test_data_tokenized_as_num = test_data_tokenized.apply(lambda x: [word2index[word] for word in x if word in word2index])\n",
        "test_dataset = ArabicDataset(list(test_data_tokenized_as_num), test_data['category'], word2index['<فراغ>'])\n",
        "y_pred__ = evaluate(gru_model,test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qtAycDRmB75",
        "outputId": "23f8a08f-bb5e-4025-d63f-0c6581ef9beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        10\n",
            "           1       0.85      0.80      0.82       145\n",
            "           2       0.69      0.90      0.78       545\n",
            "           3       0.00      0.00      0.00        17\n",
            "           4       0.56      0.56      0.56       128\n",
            "           5       0.50      0.01      0.02        82\n",
            "           6       0.00      0.00      0.00        20\n",
            "           7       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00        15\n",
            "           9       0.58      0.19      0.29        36\n",
            "\n",
            "    accuracy                           0.69      1000\n",
            "   macro avg       0.32      0.25      0.25      1000\n",
            "weighted avg       0.63      0.69      0.63      1000\n",
            "\n",
            "accuracy: 0.689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "y_pred_all = y_pred + y_pred_ + y_pred__\n",
        "y_pred_all /= 2\n",
        "y_pred_all = np.argmax(y_pred, axis=1)\n",
        "print_report(y_pred_all, test_data['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "62cd17edec06c1bcb7cce561853235234094d242005d116fab77979ddb024dcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
