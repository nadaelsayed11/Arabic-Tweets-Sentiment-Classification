{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from pyarabic.araby import tokenize\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "# from model_building import Classifier \n",
    "from pre_processing_post import processPost\n",
    "from feature_extraction import get_ngram_features, get_word_embedding_features, avg_word_vector\n",
    "from gru_model import ArabicDataset, Classifier, evaluate, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed functions\n",
    "def print_report(y_pred, y_test):\n",
    "    report = metrics.classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'ar_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# load AraVec Spacy model\n",
    "nlp = spacy.load(\"./spacy.aravec.model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./DataSet/train.csv',sep=',',header=0)\n",
    "test_data = pd.read_csv('./DataSet/dev.csv',sep=',',header=0)\n",
    "train_data['text'] = train_data['text'].apply(lambda x: processPost(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: processPost(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat vocablary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokenized = train_data['text'].apply(tokenize)\n",
    "test_data_tokenized = test_data['text'].apply(tokenize)\n",
    "#merge all the sentences in one list\n",
    "vocab = [item for sublist in train_data_tokenized for item in sublist]\n",
    "vocab = list(set(vocab))\n",
    "vocab.append('<فراغ>')\n",
    "vocab.insert(0, '<مجهول>')\n",
    "word2index = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12538, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_train_matrix = []\n",
    "for word in vocab:\n",
    "  weights_train_matrix.append(nlp(word).vector)\n",
    "\n",
    "weights_train_matrix = torch.from_numpy(np.array(weights_train_matrix))\n",
    "weights_train_matrix.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "td-idf feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load naive bayes model\n",
    "with open('./models/NaiveBayes_tfidf.sav', 'rb') as f:\n",
    "    naive_bayes_model = pickle.load(f)\n",
    "\n",
    "with open('./models/TFIDFVectorizer.sav', 'rb') as f:\n",
    "    word_vectorizer = pickle.load(f)\n",
    "\n",
    "X_test_tfidf = word_vectorizer.transform(test_data['text'])\n",
    "y_pred = naive_bayes_model.predict_proba(X_test_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_embeddings = np.array([np.array([nlp(i).vector for i in ls]) for ls in test_data[\"text\"]], dtype=object)\n",
    "_, X_test_vect_avg = avg_word_vector([], test_data_embeddings)\n",
    "\n",
    "# load SVC model\n",
    "with open('./models/RandomForest_Ara2Vec.sav', 'rb') as f:\n",
    "    svc_model = pickle.load(f)\n",
    "\n",
    "y_pred += svc_model.predict_proba(X_test_vect_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7607, 267, 4431, 4927, 9127, 12436, 1611, 1750, 6395, 8674, 4496, 7963, 5897, 6321, 1475, 2432, 7607, 267, 10169, 9314, 421, 240, 5694, 6321, 6020, 4927, 9127, 4431, 11147, 1611, 1750, 8411, 300, 10965, 7181, 1611, 7963, 6448]\n",
      "(tensor([ 7607,   267,  4431,  4927,  9127, 12436,  1611,  1750,  6395,  8674,\n",
      "         4496,  7963,  5897,  6321,  1475,  2432,  7607,   267, 10169,  9314,\n",
      "          421,   240,  5694,  6321,  6020,  4927,  9127,  4431, 11147,  1611,\n",
      "         1750,  8411,   300, 10965,  7181,  1611,  7963,  6448, 12537, 12537,\n",
      "        12537, 12537, 12537, 12537, 12537, 12537, 12537, 12537, 12537, 12537,\n",
      "        12537, 12537, 12537, 12537], dtype=torch.int32), tensor(2))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.30      0.12        70\n",
      "           1       0.12      0.56      0.20       126\n",
      "           2       0.82      0.16      0.27       804\n",
      "\n",
      "    accuracy                           0.22      1000\n",
      "   macro avg       0.34      0.34      0.20      1000\n",
      "weighted avg       0.68      0.22      0.25      1000\n",
      "\n",
      "\n",
      "Test Accuracy: 0.21899999678134918\n",
      "[[-1.922905    0.9292855   1.3081696 ]\n",
      " [-2.780834    1.2828125   1.4526263 ]\n",
      " [-0.27978054  0.13587283 -0.15097593]\n",
      " ...\n",
      " [-2.2708514   2.081671   -0.48762155]\n",
      " [-0.26863557  0.39028868 -0.23718561]\n",
      " [ 0.08276586  0.16059619 -0.13214381]]\n"
     ]
    }
   ],
   "source": [
    "#load GRU model\n",
    "with open('./models/GRU_Ara2Vec.pth', 'rb') as f: \n",
    "    # device = torch.device(\"cpu\")\n",
    "    gru_model = Classifier(weights_train_matrix)\n",
    "    gru_model.load_state_dict(torch.load(f)) \n",
    "    # gru_model.eval()\n",
    "\n",
    "\n",
    "test_data_tokenized_as_num = test_data_tokenized.apply(lambda x: [word2index[word] for word in x if word in word2index])\n",
    "print(test_data_tokenized_as_num[0])\n",
    "test_dataset = ArabicDataset(list(test_data_tokenized_as_num), test_data['stance'] + 1, word2index['<فراغ>'])\n",
    "print(test_dataset[0])\n",
    "y_pred_ = evaluate(gru_model,test_dataset)\n",
    "print(y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.04      0.08        70\n",
      "           0       0.38      0.13      0.20       126\n",
      "           1       0.82      0.97      0.89       804\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.54      0.38      0.39      1000\n",
      "weighted avg       0.74      0.80      0.75      1000\n",
      "\n",
      "accuracy: 0.799\n"
     ]
    }
   ],
   "source": [
    "y_pred /= 2\n",
    "y_pred = np.argmax(y_pred, axis=1) - 1\n",
    "print_report(y_pred, test_data['stance'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62cd17edec06c1bcb7cce561853235234094d242005d116fab77979ddb024dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
